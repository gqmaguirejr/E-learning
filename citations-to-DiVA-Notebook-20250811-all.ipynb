{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using data from citedtags.bib and fordiva.json files look up DiVA IDs for these publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some installs of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: bibtexparser in /home/maguire/.local/lib/python3.11/site-packages (1.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in /home/maguire/.local/lib/python3.11/site-packages (from bibtexparser) (2.4.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: thefuzz in /home/maguire/.local/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: python-Levenshtein in /home/maguire/.local/lib/python3.11/site-packages (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /home/maguire/.local/lib/python3.11/site-packages (from thefuzz) (3.13.0)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in /home/maguire/.local/lib/python3.11/site-packages (from python-Levenshtein) (0.27.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bibtexparser\n",
    "!pip install thefuzz python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\t# to make OS calls, here to get time zone info\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import urllib.request\n",
    "import requests\n",
    "\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "from io import StringIO\n",
    "\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# import logging\n",
    "\n",
    "import pymods\n",
    "\n",
    "import bibtexparser\n",
    "from bibtexparser.bparser import BibTexParser\n",
    "\n",
    "\n",
    "\n",
    "from thefuzz import fuzz\n",
    "\n",
    "#from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, set the flag to True\n",
    "Verbose_Flag=False\n",
    "\n",
    "# set directory to use for output of the collected words\n",
    "directory_prefix ='/tmp/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4240e5585c844155a968494c43c5f171",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Define some helper functions and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "cell_id": "48c76af5b22d4468b0b206bc0cdd411a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1667202629310,
    "source_hash": "955ec4c5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_file_older_than_x_days(file, days=1): \n",
    "    file_time = os.path.getmtime(file) \n",
    "    # Check against 24 hours \n",
    "    return ((time.time() - file_time) / 3600 > 24*days)\n",
    "\n",
    "# define a function to compute lower case Roman numeras from an integer\n",
    "def int_to_roman(num):\n",
    "    res = \"\"\n",
    "    table = [\n",
    "        (1000, \"m\"),\n",
    "        (900, \"cm\"),\n",
    "        (500, \"d\"),\n",
    "        (400, \"cd\"),\n",
    "        (100, \"c\"),\n",
    "        (90, \"xc\"),\n",
    "        (50, \"l\"),\n",
    "        (40, \"xl\"),\n",
    "        (10, \"x\"),\n",
    "        (9, \"ix\"),\n",
    "        (5, \"v\"),\n",
    "        (4, \"iv\"),\n",
    "        (1, \"i\"),\n",
    "    ]\n",
    "    for cap, roman in table:\n",
    "        d, m = divmod(num, cap)\n",
    "        res += roman * d\n",
    "        num = m\n",
    "    return res\n",
    "\n",
    "# Using the above function fill a dict with the lower case Roman numerals from 0 to 499\n",
    "# These are precomputed, so later we can simply do a lookup of the string and et the integer.\n",
    "roman_table=dict()\n",
    "for i in range(0,500):\n",
    "    roman_table[int_to_roman(i)]=i\n",
    "\n",
    "def collect_originInfo(mod_elem):\n",
    "    originInfo=dict()\n",
    "    for elem in mod_elem:\n",
    "        if elem.tag.count(\"}languageTerm\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"LanguageTerm\"]=elem.text\n",
    "        elif elem.tag.count(\"}dateIssued\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"CreatedDate\"]=elem.text\n",
    "        elif elem.tag.count(\"}dateOther\") == 1:\n",
    "            # <dateOther type=\"defence\">2018-07-26T13:00:00</dateOther>\n",
    "            # <dateOther type=\"academicTerm\">VT 2018</dateOther>\n",
    "            # <dateOther type=\"availableFrom\">2018-11-19T09:16:45</dateOther>\n",
    "            if elem.text is not None:\n",
    "                type=elem.attrib.get('type')\n",
    "                if type == 'defence':\n",
    "                    originInfo['DefenceDate']=elem.text\n",
    "                elif type == 'academicTerm':\n",
    "                    originInfo['academicTerm']=elem.text\n",
    "                elif type == 'availableFrom':\n",
    "                    originInfo['PublicationDate']=elem.text\n",
    "                else:\n",
    "                    originInfo[\"dateOther\"]=elem.text\n",
    "        elif elem.tag.count(\"}place\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"place\"]=elem.text\n",
    "        elif elem.tag.count(\"}publisher\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"publisher\"]=elem.text\n",
    "        elif elem.tag.count(\"}edition\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"edition\"]=elem.text\n",
    "        elif elem.tag.count(\"}genre\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"genre\"]=elem.text\n",
    "        else:\n",
    "            print(\"Unhandled case in collect_originInfo: {}\".format(elem))\n",
    "    return originInfo\n",
    "\n",
    "def update_key_list(diva_entry, key, name_struct):\n",
    "    current_value=diva_entry.get(key, list())\n",
    "    current_value.append(name_struct)\n",
    "    return current_value\n",
    "\n",
    "# The following have been adapted from https://www.loc.gov/marc/relators/relaterm.html\n",
    "# for use in the content of KTH, where a 'mon' is an 'examiner' and a 'ths' is a 'supervisor'\n",
    "\n",
    "MARC_Code_for_Relators = {\n",
    "    'abr': 'abridger',\n",
    "    'acp': 'art copyist',\n",
    "    'act': 'actor',\n",
    "    'adi': 'art director',\n",
    "    'adp': 'adapter',\n",
    "    'anl': 'analyst',\n",
    "    'anm': 'animator',\n",
    "    'ann': 'annotator',\n",
    "    'anc': 'announcer',\n",
    "    'ant': 'bibliographic antecedent',\n",
    "    'ape': 'appellee',\n",
    "    'apl': 'appellant',\n",
    "    'app': 'applicant',\n",
    "    'aqt': 'author in quotations or text abstracts',\n",
    "    'arc': 'architect',\n",
    "    'ard': 'artistic director',\n",
    "    'arr': 'arranger',\n",
    "    'art': 'artist',\n",
    "    'asg': 'assignee',\n",
    "    'asn': 'associated name',\n",
    "    'ato': 'autographer',\n",
    "    'att': 'attributed name',\n",
    "    'auc': 'auctioneer',\n",
    "    'aud': 'author of dialog',\n",
    "    'aue': 'audio engineer',\n",
    "    'aui': 'author of introduction',\n",
    "    'aup': 'audio producer',\n",
    "    'aus': 'screenwriter',\n",
    "    'aut': 'author',\n",
    "    'bdd': 'binding designer',\n",
    "    'bjd': 'bookjacket designer',\n",
    "    'bka': 'book artist',\n",
    "    'bkd': 'book designer',\n",
    "    'bkp': 'book producer',\n",
    "    'blw': 'blurb writer',\n",
    "    'bnd': 'binder',\n",
    "    'bpd': 'bookplate designer',\n",
    "    'brd': 'broadcaster',\n",
    "    'brl': 'braille embosser',\n",
    "    'bsl': 'bookseller',\n",
    "    'cad': 'casting director',\n",
    "    'cas': 'caster',\n",
    "    'ccp': 'conceptor',\n",
    "    'chr': 'choreographer',\n",
    "    'cli': 'client',\n",
    "    'cll': 'calligrapher',\n",
    "    'clr': 'colorist',\n",
    "    'clt': 'collotyper',\n",
    "    'cmm': 'commentator',\n",
    "    'cmp': 'composer',\n",
    "    'cmt': 'compositor',\n",
    "    'cnd': 'conductor',\n",
    "    'cng': 'cinematographer',\n",
    "    'cns': 'censor',\n",
    "    'coe': 'contestant-appellee',\n",
    "    'col': 'collector',\n",
    "    'com': 'compiler',\n",
    "    'con': 'conservator',\n",
    "    'cop': 'camera operator',\n",
    "    'cor': 'collection registrar',\n",
    "    'cos': 'contestant',\n",
    "    'cot': 'contestant-appellant',\n",
    "    'cou': 'court governed',\n",
    "    'cov': 'cover designer',\n",
    "    'cpc': 'copyright claimant',\n",
    "    'cpe': 'complainant-appellee',\n",
    "    'cph': 'copyright holder',\n",
    "    'cpl': 'complainant',\n",
    "    'cpt': 'complainant-appellant',\n",
    "    'cre': 'creator',\n",
    "    'crp': 'correspondent',\n",
    "    'crr': 'corrector',\n",
    "    'crt': 'court reporter',\n",
    "    'csl': 'consultant',\n",
    "    'csp': 'consultant to a project',\n",
    "    'cst': 'costume designer',\n",
    "    'ctb': 'contributor',\n",
    "    'cte': 'contestee-appellee',\n",
    "    'ctg': 'cartographer',\n",
    "    'ctr': 'contractor',\n",
    "    'cts': 'contestee',\n",
    "    'ctt': 'contestee-appellant',\n",
    "    'cur': 'curator',\n",
    "    'cwt': 'commentator for written text',\n",
    "    'dbd': 'dubbing director',\n",
    "    'dbp': 'distribution place',\n",
    "    'dfd': 'defendant',\n",
    "    'dfe': 'defendant-appellee',\n",
    "    'dft': 'defendant-appellant',\n",
    "    'dgc': 'degree committee member',\n",
    "    'dgg': 'degree granting institution',\n",
    "    'dgs': 'degree supervisor',\n",
    "    'dis': 'dissertant',\n",
    "    'djo': 'dj',\n",
    "    'dln': 'delineator',\n",
    "    'dnc': 'dancer',\n",
    "    'dnr': 'donor',\n",
    "    'dpc': 'depicted',\n",
    "    'dpt': 'depositor',\n",
    "    'drm': 'draftsman',\n",
    "    'drt': 'director',\n",
    "    'dsr': 'designer',\n",
    "    'dst': 'distributor',\n",
    "    'dtc': 'data contributor',\n",
    "    'dte': 'dedicatee',\n",
    "    'dtm': 'data manager',\n",
    "    'dto': 'dedicator',\n",
    "    'dub': 'dubious author',\n",
    "    'edc': 'editor of compilation',\n",
    "    'edd': 'editorial director',\n",
    "    'edm': 'editor of moving image work',\n",
    "    'edt': 'editor',\n",
    "    'egr': 'engraver',\n",
    "    'elg': 'electrician',\n",
    "    'elt': 'electrotyper',\n",
    "    'enj': 'enacting jurisdiction',\n",
    "    'eng': 'engineer',\n",
    "    'etr': 'etcher',\n",
    "    'evp': 'event place',\n",
    "    'exp': 'expert',\n",
    "    'fac': 'facsimilist',\n",
    "    'fds': 'film distributor',\n",
    "    'fld': 'field director',\n",
    "    'flm': 'film editor',\n",
    "    'fmd': 'film director',\n",
    "    'fmk': 'filmmaker',\n",
    "    'fmo': 'former owner',\n",
    "    'fmp': 'film producer',\n",
    "    'fnd': 'funder',\n",
    "    'fon': 'founder',\n",
    "    'fpy': 'first party',\n",
    "    'frg': 'forger',\n",
    "    'gdv': 'game developer',\n",
    "    'gis': 'geographic information specialist',\n",
    "    'his': 'host institution',\n",
    "    'hnr': 'honoree',\n",
    "    'hst': 'host',\n",
    "    'ill': 'illustrator',\n",
    "    'ilu': 'illuminator',\n",
    "    'ink': 'inker',\n",
    "    'ins': 'inscriber',\n",
    "    'inv': 'inventor',\n",
    "    'isb': 'issuing body',\n",
    "    'itr': 'instrumentalist',\n",
    "    'ive': 'interviewee',\n",
    "    'ivr': 'interviewer',\n",
    "    'jud': 'judge',\n",
    "    'jug': 'jurisdiction governed',\n",
    "    'lbr': 'laboratory',\n",
    "    'lbt': 'librettist',\n",
    "    'ldr': 'laboratory director',\n",
    "    'led': 'lead',\n",
    "    'lee': 'libelee-appellee',\n",
    "    'lel': 'libelee',\n",
    "    'len': 'lender',\n",
    "    'let': 'libelee-appellant',\n",
    "    'lgd': 'lighting designer',\n",
    "    'lie': 'libelant-appellee',\n",
    "    'lil': 'libelant',\n",
    "    'lit': 'libelant-appellant',\n",
    "    'lsa': 'landscape architect',\n",
    "    'lse': 'licensee',\n",
    "    'lso': 'licensor',\n",
    "    'ltg': 'lithographer',\n",
    "    'ltr': 'letterer',\n",
    "    'lyr': 'lyricist',\n",
    "    'mcp': 'music copyist',\n",
    "    'mdc': 'metadata contact',\n",
    "    'med': 'medium',\n",
    "    'mfp': 'manufacture place',\n",
    "    'mfr': 'manufacturer',\n",
    "    'mka': 'makeup artist',\n",
    "    'mod': 'moderator',\n",
    "    #'mon': 'monitor',\n",
    "    'mon': 'examiner',\n",
    "    'mrb': 'marbler',\n",
    "    'mrk': 'markup editor',\n",
    "    'msd': 'musical director',\n",
    "    'mte': 'metal-engraver',\n",
    "    'mtk': 'minute taker',\n",
    "    'mup': 'music programmer',\n",
    "    'mus': 'musician',\n",
    "    'mxe': 'mixing engineer',\n",
    "    'nan': 'news anchor',\n",
    "    'nrt': 'narrator',\n",
    "    'onp': 'onscreen participant',\n",
    "    'opn': 'opponent',\n",
    "    'osp': 'onscreen presenter',\n",
    "    'org': 'organizer',\n",
    "    'orm': 'organizer',\n",
    "    'oth': 'other',\n",
    "    'own': 'owner',\n",
    "    'pad': 'place of address',\n",
    "    'pan': 'panelist',\n",
    "    'pat': 'patron',\n",
    "    'pbd': 'publishing director',\n",
    "    'pbl': 'publisher',\n",
    "    'pdr': 'project director',\n",
    "    'pfr': 'proofreader',\n",
    "    'pgr': 'programmer',\n",
    "    'pht': 'photographer',\n",
    "    'plt': 'platemaker',\n",
    "    'pma': 'permitting agency',\n",
    "    'pmn': 'production manager',\n",
    "    'pnc': 'penciller',\n",
    "    'pop': 'printer of plates',\n",
    "    'ppm': 'papermaker',\n",
    "    'ppt': 'puppeteer',\n",
    "    'pra': 'praeses',\n",
    "    'prc': 'process contact',\n",
    "    'prd': 'production personnel',\n",
    "    'pre': 'presenter',\n",
    "    'prf': 'performer',\n",
    "    'prg': 'programmer',\n",
    "    'prm': 'printmaker',\n",
    "    'prn': 'production company',\n",
    "    'pro': 'producer',\n",
    "    'prp': 'production place',\n",
    "    'prs': 'production designer',\n",
    "    'prt': 'printer',\n",
    "    'prv': 'provider',\n",
    "    'pta': 'patent applicant',\n",
    "    'pte': 'plaintiff-appellee',\n",
    "    'pth': 'patent holder',\n",
    "    'ptf': 'plaintiff',\n",
    "    'ptt': 'plaintiff-appellant',\n",
    "    'pup': 'publication place',\n",
    "    'rbr': 'rubricator',\n",
    "    'rcd': 'recordist',\n",
    "    'rce': 'recording engineer',\n",
    "    'rcp': 'addressee',\n",
    "    'rdd': 'radio director',\n",
    "    'red': 'redaktor',\n",
    "    'ren': 'renderer',\n",
    "    'res': 'researcher',\n",
    "    'rev': 'reviewer',\n",
    "    'rpc': 'radio producer',\n",
    "    'rps': 'repository',\n",
    "    'rpt': 'reporter',\n",
    "    'rpy': 'responsible party',\n",
    "    'rsd': 'stage director',\n",
    "    'rsg': 'restager',\n",
    "    'rsr': 'restorationist',\n",
    "    'rsp': 'respondent',\n",
    "    'rst': 'respondent-appellant',\n",
    "    'rse': 'respondent-appellee',\n",
    "    'rth': 'research team head',\n",
    "    'rtm': 'research team member',\n",
    "    'rxa': 'remix artist',\n",
    "    'sad': 'scientific advisor',\n",
    "    'sce': 'scenarist',\n",
    "    'scl': 'sculptor',\n",
    "    'scr': 'scribe',\n",
    "    'sds': 'sound designer',\n",
    "    'sde': 'sound engineer',\n",
    "    'sec': 'secretary',\n",
    "    'sfx': 'special effects provider',\n",
    "    'sgd': 'stage director',\n",
    "    'sgn': 'signer',\n",
    "    'sht': 'supporting host',\n",
    "    'sll': 'seller',\n",
    "    'sng': 'singer',\n",
    "    'spk': 'speaker',\n",
    "    'spn': 'sponsor',\n",
    "    'spy': 'second party',\n",
    "    'srv': 'surveyor',\n",
    "    'std': 'set designer',\n",
    "    'stg': 'setting',\n",
    "    'stm': 'stage manager',\n",
    "    'stn': 'standards body',\n",
    "    'str': 'stereotyper',\n",
    "    'stl': 'storyteller',\n",
    "    'swd': 'software developer',\n",
    "    'tad': 'technical advisor',\n",
    "    'tau': 'television writer',\n",
    "    'tcd': 'technical director',\n",
    "    'tch': 'teacher',\n",
    "    #'ths': 'thesis advisor',\n",
    "    'ths': 'supervisor',\n",
    "    'tld': 'television director',\n",
    "    'tlg': 'television guest',\n",
    "    'tlh': 'television host',\n",
    "    'tlp': 'television producer',\n",
    "    'trc': 'transcriber',\n",
    "    'trl': 'translator',\n",
    "    'tyd': 'type designer',\n",
    "    'tyg': 'typographer',\n",
    "    'uvp': 'university place',\n",
    "    'vac': 'voice actor',\n",
    "    'vdg': 'videographer',\n",
    "    'vfx': 'visual effects provider',\n",
    "    'wac': 'writer of added commentary',\n",
    "    'wal': 'writer of added lyrics',\n",
    "    'wam': 'writer of accompanying material',\n",
    "    'wat': 'writer of added text',\n",
    "    'waw': 'writer of afterword',\n",
    "    'wdc': 'woodcutter',\n",
    "    'wde': 'wood engraver',\n",
    "    'wfs': 'writer of film story',\n",
    "    'wfw': 'writer of foreword',\n",
    "    'wft': 'writer of intertitles',\n",
    "    'win': 'writer of introduction',\n",
    "    'wit': 'witness',\n",
    "    'wpr': 'writer of preface',\n",
    "    'wst': 'writer of supplementary textual content',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to parse the MODS record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbosePrint(msg):\n",
    "    global Verbose_Flag\n",
    "    if Verbose_Flag:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def mods_records_to_dataframe(mods_records): # return a dataframe\n",
    "    n_index=0                                # index over entries\n",
    "    df=pd.DataFrame()           # define a dataframe to collect the results\n",
    "    df2=pd.DataFrame()          # define a dataframe for each entry\n",
    "    for record in mods_records:\n",
    "        n_index=n_index+1\n",
    "        diva_entry=dict()\n",
    "        diva_entry['node']=n_index\n",
    "        verbosePrint(f\"{n_index=} {record=}\")\n",
    "        verbosePrint(record.tag)\n",
    "        if record.tag.count(\"}mods\") == 1:\n",
    "            #print(\"Attribute: {}\".format(record.attrib))\n",
    "            for mod_element in record:\n",
    "                verbosePrint(mod_element)\n",
    "                if mod_element.tag.count(\"}genre\") >= 1:\n",
    "                    if mod_element.attrib.get('authority') == \"diva\" and mod_element.attrib.get('type') == \"publicationTypeCode\":\n",
    "                        diva_entry['publicationTypeCode']=mod_element.text\n",
    "                        verbosePrint(F\"publicationTypeCode= {mod_element.text}\")\n",
    "                        if mod_element.attrib.get('type') == \"publicationType\":\n",
    "                            authority=mod_element.attrib.get('authority')\n",
    "                            current_pubtype=diva_entry.get('publicationType', dict())\n",
    "                            if authority == 'diva':\n",
    "                                lang=mod_element.attrib.get('lang')\n",
    "                                if lang:\n",
    "                                    current_pubtype.update({lang: mod_element.text})\n",
    "                                diva_entry['diva_publicationType']=current_pubtype\n",
    "                            elif authority == 'svep':\n",
    "                                diva_entry['svep_publicationType']=mod_element.text\n",
    "                            elif authority == 'kev':\n",
    "                                lang=mod_element.attrib.get('lang')\n",
    "                                current_pubtype=diva_entry.get('kev_publicationType', dict())\n",
    "                                if lang:\n",
    "                                    current_pubtype.update({lang: mod_element.text})\n",
    "                                diva_entry['kev_publicationType']=current_pubtype\n",
    "                            else:\n",
    "                                print(f\"Unhandled case in publicationType: {mod_element.attrib=} {mod_element.text=}\")\n",
    "                elif mod_element.tag.count(\"}name\") == 1:\n",
    "                    # <name type=\"personal\" authority=\"kth\" xlink:href=\"u19gy7zg\">\n",
    "                    # <name type=\"corporate\" authority=\"kth\" xlink:href=\"5956\"><namePart>KTH</namePart><namePart>Skolan för datavetenskap och kommunikation (CSC)</namePart>\n",
    "                    if mod_element.attrib.get('type') == \"personal\":\n",
    "                        name_type='personal'\n",
    "                    elif mod_element.attrib.get('type') == \"corporate\":\n",
    "                        name_type='corporate'\n",
    "                    elif mod_element.attrib.get('type') == \"conference\":\n",
    "                        name_type='conference'    \n",
    "                    else:\n",
    "                        name_type='unknown'\n",
    "                    name_struct={'type': name_type}\n",
    "                    name_authority=mod_element.attrib.get('authority')\n",
    "                    if name_authority is not None:\n",
    "                        name_struct['authority']=name_authority\n",
    "                    xlink=mod_element.attrib.get('{http://www.w3.org/1999/xlink}href', None)\n",
    "                    if xlink is not None:\n",
    "                        name_struct['xlink']=xlink\n",
    "                    for elem in mod_element:\n",
    "                        if elem.tag.count(\"}namePart\") == 1:\n",
    "                            # personal name\n",
    "                            namePart_type=elem.attrib.get('type')\n",
    "                            if namePart_type == 'family':\n",
    "                                name_struct['family']=elem.text\n",
    "                            elif namePart_type == 'given':\n",
    "                                name_struct['given']=elem.text\n",
    "                            elif namePart_type == 'termsOfAddress':\n",
    "                                # <namePart type=\"termsOfAddress\">professor</namePart>\n",
    "                                name_struct['termsOfAddress']=elem.text\n",
    "                            elif name_type =='conference':\n",
    "                                name_struct['conference']=elem.text\n",
    "                            else:\n",
    "                                orglevel=0\n",
    "                                orglevels=dict()\n",
    "                                for org in elem:\n",
    "                                    orglevelname=f\"L{orglevel}\"\n",
    "                                    orglevels[orglevelname]=elem.text\n",
    "                                    orglevel=orglevel+1\n",
    "                                if orglevels:\n",
    "                                    name_struct['orglevels']=orglevels\n",
    "                        elif elem.tag.count(\"}role\") == 1:\n",
    "                            # <role><roleTerm type=\"code\" authority=\"marcrelator\">pbl</roleTerm>\n",
    "                            for role in elem:\n",
    "                                role_type=role.attrib.get('type')\n",
    "                                role_authority=role.attrib.get('authority')\n",
    "                                # the codes come from https://www.loc.gov/marc/relators/relaterm.html\n",
    "                                if role_type=='code'and role_authority=='marcrelator':\n",
    "                                    if role.text in MARC_Code_for_Relators:\n",
    "                                        name_struct['role']=MARC_Code_for_Relators[role.text]\n",
    "                                    else:\n",
    "                                        print(f'Unhandled role: {role.text}')\n",
    "                        elif elem.tag.count(\"}affiliation\") == 1:\n",
    "                            # <affiliation>KTH, Kommunikationssystem, CoS</affiliation>\n",
    "                            name_struct['affiliation']=elem.text\n",
    "                        elif elem.tag.count(\"}description\") == 1:\n",
    "                            # <description>orcid.org=0000-0002-6066-746X</description>\n",
    "                            name_struct['description']=elem.text\n",
    "                        else:\n",
    "                            print('Unhandled case of role')\n",
    "                    name_role=name_struct.get('role', None)\n",
    "                    if name_role is not None:\n",
    "                        # There should only be one examiner, but we support several\n",
    "                        # There can be multiple supervisors and authors\n",
    "                        if name_role in ['examiner',  'supervisor', 'opponent',  'applicant', 'architect', 'author', 'author of dialog',\n",
    "                                         'author of introduction', 'author in quotations or text abstracts', 'editor', 'artist',\n",
    "                                         'cinematographer', 'commentator',\n",
    "                                         'commentator for written text', 'contributor', 'cover designer',\n",
    "                                         'creator', 'curator',  'designer', 'director', 'dissertant',\n",
    "                                         'filmmaker',  'illustrator', 'inventor',\n",
    "                                         'narrator', 'photographer', 'producer', 'project director', 'programmer',\n",
    "                                         'publisher', 'researcher', 'screenwriter', 'translator', 'writer of accompanying material'\n",
    "                                         ]:\n",
    "                            diva_entry[name_role] = update_key_list(diva_entry, name_role, name_struct)\n",
    "                        elif name_role == 'other':\n",
    "                            if name_struct.get('type', None) == 'corporate':\n",
    "                                if name_struct.get('description', None) == 'Research Group':\n",
    "                                    diva_entry['research group']= name_struct\n",
    "                                else:\n",
    "                                    print(f'Unhandled name - other: {name_struct} with role: {name_role}')\n",
    "                            else:\n",
    "                                print(f'Unhandled name - unknown: {name_struct} with role: {name_role}')\n",
    "                    else:\n",
    "                        if name_type == 'conference':\n",
    "                            diva_entry['conference']=name_struct.get('conference')\n",
    "                        elif name_type == 'personal': # as it an author but role not explicit\n",
    "                            diva_entry['author'] = update_key_list(diva_entry, 'author', name_struct)\n",
    "                        else:\n",
    "                            print(f'Unhandled name - with name_type: {name_type=} {name_struct=} {diva_entry=}')\n",
    "                elif mod_element.tag.count(\"}titleInfo\") == 1:\n",
    "                    #current_titleInfo=diva_entry.get('titleInfo', dict())\n",
    "                    #diva_entry['titleInfo']=current_titleInfo\n",
    "                    lang=mod_element.attrib.get('lang')\n",
    "                    for elem in mod_element:\n",
    "                        if elem.tag.count(\"}title\") == 1:\n",
    "                            #current_title=current_titleInfo.get('title', dict())\n",
    "                            current_title=diva_entry.get('title', dict())\n",
    "                            current_title.update({lang: elem.text})\n",
    "                            diva_entry['title']=current_title\n",
    "                        elif elem.tag.count(\"}subTitle\") == 1:\n",
    "                            #current_subtitle=current_titleInfo.get('subtitle', dict())\n",
    "                            current_subtitle=diva_entry.get('subtitle', dict())\n",
    "                            current_subtitle.update({lang: elem.text})\n",
    "                            diva_entry['subtitle']=current_subtitle\n",
    "                        else:\n",
    "                            print(\"Unhandled case in titleInfo\")\n",
    "                elif mod_element.tag.count(\"}language\") == 1:\n",
    "                    i=0\n",
    "                    temp_dict=dict()\n",
    "                    for elem in mod_element:\n",
    "                        i=i+1\n",
    "                        if elem.tag.count(\"}languageTerm\") == 1:\n",
    "                            if elem.text is not None:\n",
    "                                name='languageTerm_{0}'.format(i)\n",
    "                                temp_dict[name]=[elem.text]\n",
    "                        elif elem.tag.count(\"}dateIssued\") == 1:\n",
    "                            if elem.text is not None:\n",
    "                                verbosePrint(\"dateIssued: \".format(elem.text))\n",
    "                                name='dateIssued_{0}'.format(i)\n",
    "                                temp_dict[name]=[elem.text]\n",
    "                        elif elem.tag.count(\"}dateOther\") == 1:\n",
    "                            if elem.text is not None:\n",
    "                                verbosePrint(\"dateOther: {0}{1}\".format(elem.attrib, elem.text))\n",
    "                                name='dateOther_{0}'.format(i)\n",
    "                                temp_dict[name]=[elem.text]\n",
    "                        else:\n",
    "                            verbosePrint(\"mod_emem[\" + str(i) +\"]\".format(elem))\n",
    "                            name='language_{0}'.format(i)\n",
    "                            temp_dict[name]=[elem.text]\n",
    "                    verbosePrint(\"temp_dict={}\".format(temp_dict))\n",
    "                elif mod_element.tag.count(\"}originInfo\") == 1:\n",
    "                    diva_entry['originInfo']=collect_originInfo(mod_element)\n",
    "                    diva_entry['Year']=diva_entry['originInfo'].get(\"CreatedDate\", 'Unknown year')\n",
    "                elif mod_element.tag.count(\"}identifier\") == 1:\n",
    "                    if mod_element.text is not None and mod_element.attrib.get('type') in ['libris', 'articleId', 'url', 'doi', 'pmid', 'isi', 'scopus', 'uri', 'isrn', 'isbn', 'local']:\n",
    "                        diva_entry[mod_element.attrib.get('type')]=mod_element.text\n",
    "                    else:\n",
    "                        print(f\"unexpected identifier type: {mod_element.attrib.get('type')}\")\n",
    "                elif mod_element.tag.count(\"}abstract\") == 1:\n",
    "                    current_abstracts=diva_entry.get('abstract', dict())\n",
    "                    lang=mod_element.attrib.get('lang')\n",
    "                    if lang == '-1' or not isinstance(lang , str):\n",
    "                        # print(f\"{lang=} in {diva_entry=}\")\n",
    "                        lang='swe' # corect the entry in  diva2:905489\n",
    "                    current_abstracts.update({lang: mod_element.text})\n",
    "                    diva_entry['abstract']=current_abstracts\n",
    "                elif mod_element.tag.count(\"}subject\") == 1:\n",
    "                    #<subject lang=\"eng\" xlink:href=\"9895\"><topic>Master of Science - Computer Science</topic><genre>Educational program</genre></subject>\n",
    "                    #<subject lang=\"swe\" xlink:href=\"9895\"><topic>Teknologie masterexamen - Datalogi</topic><genre>Educational program</genre></subject>\n",
    "                    #<subject lang=\"eng\" xlink:href=\"10280\"><topic>Computer Science</topic><genre>Subject/course</genre></subject>\n",
    "                    #<subject lang=\"swe\" xlink:href=\"10280\"><topic>Datalogi</topic><genre>Subject/course</genre></subject>\n",
    "                    xlink=mod_element.attrib.get('{http://www.w3.org/1999/xlink}href', None)\n",
    "                    xlinks=mod_element.attrib.get('xlink', None)\n",
    "                    authority=mod_element.attrib.get('authority', None)\n",
    "                    if authority:\n",
    "                        subject=f'{authority}'\n",
    "                    elif xlink:    \n",
    "                        subject='xlink' \n",
    "                    elif xlinks:    \n",
    "                        subject='xlinks' \n",
    "                    else:\n",
    "                        subject='keywords'\n",
    "                    lang=mod_element.attrib.get('lang')\n",
    "                    current_subject=diva_entry.get(subject, dict())\n",
    "                    topics=current_subject.get(lang, list())\n",
    "                    for elem in mod_element:\n",
    "                        if elem.tag.count(\"}topic\") == 1:\n",
    "                            topics.append(elem.text)\n",
    "                            current_subject.update({lang: topics})\n",
    "                    diva_entry[subject]=current_subject\n",
    "                elif mod_element.tag.count(\"}recordInfo\") == 1:\n",
    "                    #<recordInfo>\n",
    "                    #<recordOrigin>u1d13i2c</recordOrigin>\n",
    "                    #<recordContentSource>kth</recordContentSource>\n",
    "                    #<recordCreationDate>2019-06-26</recordCreationDate>\n",
    "                    #<recordChangeDate>2022-06-26</recordChangeDate>\n",
    "                    #<recordIdentifier>diva2:1330685</recordIdentifier>\n",
    "                    #</recordInfo>\n",
    "                    current_recordInfo=diva_entry.get('recordInfo', dict())\n",
    "                    for elem in mod_element:\n",
    "                        #if elem.text is not None:\n",
    "                        #    print(f\"in recordInfo {elem.text}\")\n",
    "                        if elem.tag.count(\"}recordOrigin\") == 1:\n",
    "                            current_recordInfo[\"recordOrigin\"]=elem.text\n",
    "                        elif elem.tag.count(\"}recordContentSource\") == 1:\n",
    "                            current_recordInfo[\"recordContentSource\"]=elem.text\n",
    "                        elif elem.tag.count(\"}recordCreationDate\") == 1:\n",
    "                            current_recordInfo[\"recordCreationDate\"]=elem.text\n",
    "                        elif elem.tag.count(\"}recordChangeDate\") == 1:\n",
    "                            current_recordInfo[\"recordChangeDate\"]=elem.text\n",
    "                        elif elem.tag.count(\"}recordIdentifier\") == 1:\n",
    "                            current_recordInfo[\"recordIdentifier\"]=elem.text\n",
    "                        else:\n",
    "                            print(\"unhandled case in recordInfo\")\n",
    "                        diva_entry['recordInfo']=current_recordInfo\n",
    "                elif mod_element.tag.count(\"}location\") == 1:\n",
    "                    # <location><url displayLabel=\"fulltext\" note=\"free\" access=\"raw object\">http://kth.diva-portal.org/smash/get/diva2:821850/FULLTEXT01.pdf</url></location>\n",
    "                    current_location=diva_entry.get('location', dict())\n",
    "                    for elem in mod_element:\n",
    "                        if elem.tag.count(\"}url\") == 1:\n",
    "                            label=elem.attrib.get('displayLabel', None)\n",
    "                            if label:\n",
    "                                if elem.text:\n",
    "                                    if elem.text == diva_entry['title'].get('eng', None) or elem.text == diva_entry['title'].get('swe', None):\n",
    "                                        cl=current_location.get('url', list())\n",
    "                                        cl.append(elem.text)\n",
    "                                        current_location['url']=cl\n",
    "                                    elif label.find(',') >= 0 or label.startswith('Betydelsen av skuggning'):\n",
    "                                        cl=current_location.get('url', list())\n",
    "                                        cl.append(elem.text)\n",
    "                                        current_location['url']=cl\n",
    "                                    elif label == 'Fulltext' or label == 'fulltext' or label == 'Kandidatexjobb i Medieteknik (DM129X) år 2010':\n",
    "                                        cl=current_location.get('url', list())\n",
    "                                        cl.append(elem.text)\n",
    "                                        current_location['url']=cl\n",
    "                                    else:\n",
    "                                        cl=current_location.get('other_links', list())\n",
    "                                        cl.append(label+' url: '+elem.text)\n",
    "                                        current_location['other_links']=cl\n",
    "                                else:\n",
    "                                    cl=current_location.get(label, list())\n",
    "                                    cl.append(\"\")\n",
    "                                    current_location[label]=cl\n",
    "                            else:\n",
    "                                if elem.text:\n",
    "                                    cl=current_location.get('url', list())\n",
    "                                    cl.append(elem.text)\n",
    "                                    current_location['url']=cl\n",
    "                                else:\n",
    "                                    cl=current_location.get('url', list())\n",
    "                                    cl.append(\"\")\n",
    "                                    current_location['url']=cl\n",
    "                        else:\n",
    "                            print(\"Unhandled case in }location\")\n",
    "                        diva_entry['location']=current_location\n",
    "                elif mod_element.tag.count(\"}typeOfResource\") == 1:\n",
    "                    # <typeOfResource>text</typeOfResource>\n",
    "                    diva_entry['typeOfResource']=mod_element.text\n",
    "                elif mod_element.tag.count(\"}relatedItem\") == 1:            \n",
    "                    #<relatedItem type=\"series\">\n",
    "                    #  <titleInfo><title>Trita-ICT-COS</title></titleInfo>\n",
    "                    #  <identifier type=\"issn\">1653-6347</identifier>\n",
    "                    #  <identifier type=\"local\">247</identifier>\n",
    "                    #  <identifier type=\"issue number\">COS/CCS 2007-24</identifier>\n",
    "                    #</relatedItem>\n",
    "                    relatedItemType=mod_element.attrib.get('type', None)\n",
    "                    if not relatedItemType:\n",
    "                        relatedItemType='relatedItem'\n",
    "                    titleInfo=dict()\n",
    "                    for elem in mod_element:\n",
    "                        if relatedItemType == 'host':\n",
    "                            titleInfo[\"host\"]='host'\n",
    "                        if elem is not None:\n",
    "                            if elem.tag.count(\"}titleInfo\") == 1:\n",
    "                                for title in elem:\n",
    "                                    if title is not None and title.text:\n",
    "                                        titleInfo[\"title\"]=title.text\n",
    "                            elif elem.tag.count(\"}identifier\") >= 1:\n",
    "                                for subelem in elem:\n",
    "                                    identifier_type=subelem.attrib.get('type', None)\n",
    "                                    titleInfo[f\"{identifier_type}\"]=subelem.text\n",
    "                            elif elem.tag.count(\"}note\") == 1:\n",
    "                                note_type=elem.attrib.get('type', None)\n",
    "                                if note_type is None:\n",
    "                                    note_type='note'\n",
    "                                if elem.text is not None:\n",
    "                                    titleInfo[note_type]=elem.text\n",
    "                            elif elem.tag.count(\"}part\") == 1:\n",
    "                                for subelem in elem:\n",
    "                                    if subelem.tag == 'detail':\n",
    "                                        detail_type=subelem.attrib.get('type', None)\n",
    "                                        if detail_type == 'volume':\n",
    "                                            for subsubelem in subelem:\n",
    "                                                if subsubelem.tag == 'number':\n",
    "                                                    titleInfo[f\"volume\"]=subsubelem.text\n",
    "                                        if detail_type == 'issue':\n",
    "                                            for subsubelem in subelem:\n",
    "                                                if subsubelem.tag == 'number':\n",
    "                                                    titleInfo[f\"issue\"]=subsubelem.text\n",
    "                            elif elem.tag.count(\"}genre\") == 1:\n",
    "                                if elem.text is not None:\n",
    "                                    titleInfo['genre']=elem.text\n",
    "                                else:\n",
    "                                    elemattr=elem.attrib.get('type', None)\n",
    "                                    if elemattr:\n",
    "                                        print(f\"genre case in relatedItem for {elem=} with type {elemattr}\")\n",
    "                                    else:\n",
    "                                        print(f\"genre case in relatedItem for {elem=}\")\n",
    "                            else:\n",
    "                                print(f\"Unhanded case in relatedItem for {elem=}\")\n",
    "                    diva_entry[relatedItemType]=titleInfo\n",
    "                elif mod_element.tag.count(\"}physicalDescription\") == 1:\n",
    "                    #<physicalDescription>\n",
    "                    #  <form authority=\"marcform\">electronic</form>\n",
    "                    #  <extent>xii,74</extent></physicalDescription>\n",
    "                    for elem in mod_element:\n",
    "                        if elem.tag.count(\"}extent\") == 1:\n",
    "                            diva_entry['Pages']=elem.text\n",
    "                elif mod_element.tag.count(\"}note\") == 1:\n",
    "                    #<note type=\"level\" lang=\"swe\">Självständigt arbete på avancerad nivå (masterexamen)</note>\n",
    "                    #<note type=\"universityCredits\" lang=\"swe\">20 poäng / 30 hp</note>\n",
    "                    #<note type=\"cooperation\">Saab AB</note>\n",
    "                    notetype=mod_element.attrib.get('type', None)\n",
    "                    if not notetype:\n",
    "                        continue\n",
    "                    elif notetype in ['level', 'universityCredits', 'cooperation', 'venue',  'funder', 'papers',\n",
    "                                      'sustainableDevelopment', 'publicationStatus', 'creatorCount',\n",
    "                                      'version identification'\n",
    "]:\n",
    "                        diva_entry[notetype]=mod_element.text\n",
    "\n",
    "                    elif notetype in ['degree', 'thesis', 'patent', 'project']:\n",
    "                        diva_entry[f\"{notetype}_note\"]=mod_element.text\n",
    "                    elif notetype == 'publicationChannel':\n",
    "                        diva_entry[\"degree_publicationChannel\"]=mod_element.text\n",
    "                    else:\n",
    "                        print(f\"unhandled case of note for {notetype}\")\n",
    "                else:\n",
    "                    print(\"Unhandled case in mod mod_element={}\".format(mod_element))\n",
    "        try:\n",
    "            df2 = pd.json_normalize(diva_entry)\n",
    "        except BaseException as err:\n",
    "            print(f\"Unexpected {err=}, {type(err)=} with {diva_entry=}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df=pd.concat([df, df2], axis=0, join='outer', ignore_index = True, verify_integrity=False)\n",
    "        except BaseException as err:\n",
    "            print(f\"Unexpected {err=}, {type(err)=} with {df2=}\")\n",
    "            continue\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from the fordiva.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u1XXXXXX\n"
     ]
    }
   ],
   "source": [
    "fordiva_filename=directory_prefix+'fordiva.json'\n",
    "user_info=dict()\n",
    "try:\n",
    "    with open(fordiva_filename, 'r', encoding='utf-8') as json_FH:\n",
    "        json_string=json_FH.read()\n",
    "        user_info=json.loads(json_string)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {fordiva_filename}\")\n",
    "kthid=user_info['Author1']['Local User Id']\n",
    "print(f\"{kthid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing purposes we will use my KTHID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "kthid='u1d13i2c'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get information from the citedtags.bib file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ENTRYTYPE': 'article',\n",
      "  'ID': 'ioannidis_coherent_1991_pub',\n",
      "  'abstract': 'This memo describes the Coherent File Distribution Protocol '\n",
      "              '(CFDP). This is an Experimental Protocol for the Internet '\n",
      "              'community. It does not specify an Internet standard.',\n",
      "  'author': 'Ioannidis, J. and Maguire, G.',\n",
      "  'doi': '10.17487/RFC1235',\n",
      "  'issn': '2070-1721',\n",
      "  'journal': 'Internet Request for Comments',\n",
      "  'month': 'June',\n",
      "  'title': 'Coherent {File} {Distribution} {Protocol}',\n",
      "  'url': 'http://www.rfc-editor.org/rfc/rfc1235.txt',\n",
      "  'volume': 'RFC 1235 (Experimental)',\n",
      "  'year': '1991'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'maguire_jr_new_2014_pub',\n",
      "  'abstract': 'As the most advantageous total hip arthroplasty (THA) operation '\n",
      "              'is the first, timely replacement of only the liner is socially '\n",
      "              'and economically important because the utilization of THA is '\n",
      "              'increasing as younger and more active patients are receiving '\n",
      "              'implants and they are living longer. Automatic algorithms were '\n",
      "              'developed to infer liner wear by estimating the separation '\n",
      "              'between the acetabular cup and femoral component head given a '\n",
      "              'computed tomography (CT) volume. Two series of CT volumes of a '\n",
      "              'hip phantom were acquired with the femoral component head '\n",
      "              'placed at 14 different positions relative to the acetabular '\n",
      "              'cup. The mean and standard deviation (SD) of the diameter of '\n",
      "              'the acetabular cup and femoral component head, in addition to '\n",
      "              'the range of error in the expected wear values and the '\n",
      "              'repeatability of all the measurements, were calculated. The '\n",
      "              'algorithms resulted in a mean (±SD) for the diameter of the '\n",
      "              'acetabular cup of 54.21 (±0.011) mm and for the femoral '\n",
      "              'component head of 22.09 (±0.02) mm. The wear error was '\n",
      "              '±0.1\\u2009mm and the repeatability was 0.077\\u2009mm. This '\n",
      "              'approach is applicable clinically as it utilizes readily '\n",
      "              'available computed tomography imaging systems and requires only '\n",
      "              'five minutes of human interaction.',\n",
      "  'author': 'Maguire Jr., Gerald Q. and Noz, Marilyn E. and Olivecrona, Henrik '\n",
      "            'and Zeleznik, Michael P. and Weidenhielm, Lars',\n",
      "  'doi': '10.1155/2014/528407',\n",
      "  'issn': '2356-6140, 1537-744X',\n",
      "  'journal': 'The Scientific World Journal',\n",
      "  'language': 'english',\n",
      "  'pages': '1--9',\n",
      "  'shorttitle': 'A {New} {Automated} {Way} to {Measure} {Polyethylene} {Wear} '\n",
      "                'in {THA} {Using} a {High} {Resolution} {CT} {Scanner}',\n",
      "  'title': 'A {New} {Automated} {Way} to {Measure} {Polyethylene} {Wear} in '\n",
      "           '{THA} {Using} a {High} {Resolution} {CT} {Scanner}: {Method} and '\n",
      "           '{Analysis}',\n",
      "  'url': 'http://www.hindawi.com/journals/tswj/2014/528407/',\n",
      "  'volume': '2014',\n",
      "  'year': '2014'},\n",
      " {'ENTRYTYPE': 'inproceedings',\n",
      "  'ID': 'farshin_make_2019_pub',\n",
      "  'address': 'Dresden, Germany',\n",
      "  'author': 'Farshin, Alireza and Roozbeh, Amir and Maguire, Gerald Q. and '\n",
      "            'Kostić, Dejan',\n",
      "  'booktitle': 'Proceedings of the {Fourteenth} {EuroSys} {Conference} 2019 '\n",
      "               \"{CD}-{ROM} on {ZZZ} - {EuroSys} '19\",\n",
      "  'doi': '10.1145/3302424.3303977',\n",
      "  'isbn': '978-1-4503-6281-8',\n",
      "  'language': 'english',\n",
      "  'pages': '1--17',\n",
      "  'publisher': 'ACM Press',\n",
      "  'title': 'Make the {Most} out of {Last} {Level} {Cache} in {Intel} '\n",
      "           '{Processors}',\n",
      "  'url': 'http://dl.acm.org/citation.cfm?doid=3302424.3303977',\n",
      "  'year': '2019'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'kim_small-mass_2016_pub',\n",
      "  'abstract': 'Neutrino oscillation experiments presently suggest that '\n",
      "              'neutrinos have a small but finite mass. If neutrinos have mass, '\n",
      "              'there should be a Lorentz frame in which they can be brought to '\n",
      "              'rest. This paper discusses how Wigner’s little groups can be '\n",
      "              'used to distinguish between massive and massless particles. We '\n",
      "              'derive a representation of the SL(2,c) group which separates '\n",
      "              'out the two sets of spinors: one set is gauge dependent and the '\n",
      "              'other set is gauge invariant and represents polarized '\n",
      "              'neutrinos. We show that a similar calculation can be done for '\n",
      "              'the Dirac equation. In the large-momentum/zero-mass limit, the '\n",
      "              'Dirac spinors can be separated into large and small components. '\n",
      "              'The large components are gauge invariant, while the small '\n",
      "              'components are not. These small components represent spin-1/2 '\n",
      "              'non-zero-mass particles. If we renormalize the large '\n",
      "              'components, these gauge invariant spinors represent the '\n",
      "              'polarization of neutrinos. Massive neutrinos cannot be '\n",
      "              'invariant under gauge transformations.',\n",
      "  'author': 'Kim, Y. S. and Maguire, G. Q. and Noz, M. E.',\n",
      "  'doi': '10.1155/2016/1847620',\n",
      "  'issn': '1687-7357, 1687-7365',\n",
      "  'journal': 'Advances in High Energy Physics',\n",
      "  'language': 'english',\n",
      "  'pages': '1--7',\n",
      "  'title': 'Do {Small}-{Mass} {Neutrinos} {Participate} in {Gauge} '\n",
      "           '{Transformations}?',\n",
      "  'url': 'http://www.hindawi.com/journals/ahep/2016/1847620/',\n",
      "  'volume': '2016',\n",
      "  'year': '2016'},\n",
      " {'ENTRYTYPE': 'misc',\n",
      "  'ID': 'verardo2023fmmheadenhancingautoencoderbasedecg_pub',\n",
      "  'archiveprefix': 'arXiv',\n",
      "  'author': 'Giacomo Verardo and Magnus Boman and Samuel Bruchfeld and Marco '\n",
      "            'Chiesa and Sabine Koch and Gerald Q. Maguire Jr. and Dejan Kostic',\n",
      "  'eprint': '2310.05848',\n",
      "  'primaryclass': 'cs.LG',\n",
      "  'title': '{FMM-Head}: Enhancing Autoencoder-based {ECG} anomaly detection '\n",
      "           'with prior knowledge',\n",
      "  'url': 'https://arxiv.org/abs/2310.05848',\n",
      "  'year': '2023'},\n",
      " {'ENTRYTYPE': 'patent',\n",
      "  'ID': 'US7107453B2_pub',\n",
      "  'abstract': 'Systems and methods for generating and decoding authenticatable '\n",
      "              'graphical bar codes are described. In one aspect, a '\n",
      "              'corroborative signed message is generated from information to '\n",
      "              'be encoded, and a base image is modulated with a graphical '\n",
      "              'encoding of the signed message to produce a marked image. In '\n",
      "              'another aspect, a signed message is extracted from a marked '\n",
      "              'image based upon a comparison of the marked image and a base '\n",
      "              'image. The extracted signed message is decoded to produce a '\n",
      "              'decoded message. Information encoded in the marked image is '\n",
      "              'extracted from the decoded message and authenticated.',\n",
      "  'author': 'Yen, Jonathan and Maguire Jr., Gerald Q. and Saw, Chit Wei and '\n",
      "            'Yihong, Xu',\n",
      "  'holder': 'Hewlett-Packard Development Company, L.P.',\n",
      "  'month': 'December',\n",
      "  'nationality': 'United States',\n",
      "  'note': '{Granted US 7107453 B2 (2006-09-12), EP 1340188-B1 (2007-03-07); JP '\n",
      "          '4495908-B2 (2010-07-07)}',\n",
      "  'number': '7107453B2',\n",
      "  'title': 'Authenticatable graphical bar codes',\n",
      "  'url': 'https://patentimages.storage.googleapis.com/cc/f0/90/578b04737a117e/US7107453.pdf',\n",
      "  'year': '2000'},\n",
      " {'ENTRYTYPE': 'patent',\n",
      "  'ID': 'US12111768B2_pub',\n",
      "  'abstract': 'A method and device for controlling memory handling in a '\n",
      "              'processing system comprising a cache shared between a plurality '\n",
      "              'of processing units, wherein the cache comprises a plurality of '\n",
      "              'cache portions. The method comprises obtaining first '\n",
      "              'information pertaining to an allocation of a first memory '\n",
      "              'portion of a memory to a first application, an allocation of a '\n",
      "              'first processing unit of the plurality of processing units to '\n",
      "              'the first application, and an association between a first cache '\n",
      "              'portion of the plurality of cache portions and the first '\n",
      "              'processing unit. The method further comprises reconfiguring a '\n",
      "              'mapping configuration based on the obtained first information, '\n",
      "              'and controlling a providing of first data associated with the '\n",
      "              'first application to the first cache portion from the first '\n",
      "              'memory portion using the reconfigured mapping configuration. ',\n",
      "  'author': 'Roozbeh, Amir and Farshin, Alireza and Kostić, Dejan and Maguire '\n",
      "            'Jr., Gerald Q.',\n",
      "  'holder': 'Telefonaktiebolaget LM Ericsson AB',\n",
      "  'keywords': 'memory handling, shared cache',\n",
      "  'month': 'February',\n",
      "  'nationality': 'United States',\n",
      "  'note': '{Granted US12111768 B2 (2024-10-08)}',\n",
      "  'number': '12111768',\n",
      "  'title': 'Methods and devices for controlling memory handling',\n",
      "  'year': '2020'},\n",
      " {'ENTRYTYPE': 'software',\n",
      "  'ID': '10.5281/zenodo.4435970_pub',\n",
      "  'abstract': ' <p>This is the artifact for the “PacketMill: Toward per-core '\n",
      "              '100-Gbps Networking” paper published at ASPLOS’21.</p> '\n",
      "              '<p>PacketMill is a system that optimizes the performance of '\n",
      "              'network functions via holistic inter-stack optimizations. More '\n",
      "              'specifically, PacketMill provides a new metadata management '\n",
      "              'model, called X-Change, enabling the packet processing '\n",
      "              'frameworks to provide their custom buffer to DPDK and fully '\n",
      "              'bypass rte_mbuf. Additionally, PacketMill performs a set of '\n",
      "              'source-code \\\\& intermediate representation (IR) code '\n",
      "              'optimizations.</p> <p>Our paper’s artifact contains the source '\n",
      "              'code, the experimental workflow, and additional information to '\n",
      "              '(i) set upPacketMill \\\\& its testbed, (ii) perform some of the '\n",
      "              'experiments presented in the paper, and (iii) validates the '\n",
      "              'reusability \\\\& effectiveness of PacketMill.</p> <p>For more '\n",
      "              'information, please refer to '\n",
      "              'https://github.com/aliireza/packetmill</p> {\\\\@@par }',\n",
      "  'author': 'Farshin, Alireza and Barbette, Tom and Roozbeh, Amir and Maguire '\n",
      "            'Jr., Gerald Q. and Kostić, Dejan',\n",
      "  'keywords': 'DPDK., FastClick, LLVM, Middleboxes, Packet Processing, '\n",
      "              'PacketMill, X-Change',\n",
      "  'publisher': 'Association for Computing Machinery',\n",
      "  'title': '{PacketMill: Toward Per-Core 100-Gbps Networking - Artifact for '\n",
      "           \"ASPLOS'21}\",\n",
      "  'url': 'https://doi.org/10.5281/zenodo.4435970',\n",
      "  'year': '2021'}]\n"
     ]
    }
   ],
   "source": [
    "bibtext_filename=directory_prefix+'citedtags.bib'\n",
    "bibtex_string=''\n",
    "try:\n",
    "    with open(bibtext_filename, 'r', encoding='utf-8') as bibtex_FH:\n",
    "        bibtex_string=bibtex_FH.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {bibtext_filename}\")\n",
    "\n",
    "if Verbose_Flag:\n",
    "    print(\"read bibtex file: {}\".format(d))\n",
    "\n",
    "# Create a parser object\n",
    "parser = bibtexparser.bparser.BibTexParser(ignore_nonstandard_types=False)\n",
    "\n",
    "# Use this custom parser to load the data\n",
    "bib_database = bibtexparser.loads(bibtex_string, parser=parser)\n",
    "\n",
    "# The 'entries' list will now correctly contain all your data\n",
    "pprint(bib_database.entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch a user's data from DiVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url=https://kth.diva-portal.org/smash/export.jsf?format=mods&addFilename=true&aq=[[{\"personId\":\"u1d13i2c\"}]]&aqe=[]&aq2=[[]]&onlyFullText=false&noOfRows=5000&sortOrder=title_sort_asc&sortOrder2=title_sort_asc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(336, 61)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mods_kthid(kthid, filename):\n",
    "        url='https://kth.diva-portal.org/smash/export.jsf?format=mods&addFilename=true&aq=[[{\"personId\":\"'+f\"{kthid}\"+'\"}]]&aqe=[]&aq2=[[]]&onlyFullText=false&noOfRows=5000&sortOrder=title_sort_asc&sortOrder2=title_sort_asc'\n",
    "        print(\"url={}\".format(url))\n",
    "        req = urllib.request.Request(url)\n",
    "        try:\n",
    "            with urllib.request.urlopen(req) as response:\n",
    "                data_str=response.read()\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(e.code)\n",
    "            print(e.read())\n",
    "            raise\n",
    "\n",
    "        with open(filename, \"wb\") as mods_data_file:\n",
    "            mods_data_file.write(data_str)\n",
    "        mods_records = pymods.MODSReader(filename)\n",
    "        return mods_records\n",
    "\n",
    "\n",
    "filename=f\"/tmp/{kthid}-diva-mods\"\n",
    "mods_records=get_mods_kthid(kthid, filename)\n",
    "user_df=mods_records_to_dataframe(mods_records)\n",
    "user_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to do normalization and matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Match Process ---\n",
      "\n",
      "Processing BibTeX key: ioannidis_coherent_1991_pub\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.17487/RFC1235'\n",
      "    - Normalized .bib value: '10.17487/rfc1235'\n",
      "    -> SUCCESS: Found DiVA ID diva2:461420\n",
      "Processing BibTeX key: maguire_jr_new_2014_pub\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1155/2014/528407'\n",
      "    - Normalized .bib value: '10.1155/2014/528407'\n",
      "    -> SUCCESS: Found DiVA ID diva2:690828\n",
      "Processing BibTeX key: farshin_make_2019_pub\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1145/3302424.3303977'\n",
      "    - Normalized .bib value: '10.1145/3302424.3303977'\n",
      "    -> SUCCESS: Found DiVA ID diva2:1291291\n",
      "Processing BibTeX key: kim_small-mass_2016_pub\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1155/2016/1847620'\n",
      "    - Normalized .bib value: '10.1155/2016/1847620'\n",
      "    -> SUCCESS: Found DiVA ID diva2:948742\n",
      "Processing BibTeX key: verardo2023fmmheadenhancingautoencoderbasedecg_pub\n",
      "Processing BibTeX key: US7107453B2_pub\n",
      "Processing BibTeX key: US12111768B2_pub\n",
      "Processing BibTeX key: 10.5281/zenodo.4435970_pub\n",
      "--------------------\n",
      "\n",
      "--- Final Matching Results ---\n",
      "{'10.5281/zenodo.4435970_pub': {'DiVA_ID': 'diva2:1527198',\n",
      "                                'match_method': 'Software link (url: '\n",
      "                                                'https://dl.acm.org/do/10.5281/zenodo.4435970/abs/)'},\n",
      " 'US12111768B2_pub': {'DiVA_ID': 'diva2:1926019',\n",
      "                      'match_method': 'Fuzzy Title (Score: 100)'},\n",
      " 'US7107453B2_pub': {'DiVA_ID': 'diva2:456465',\n",
      "                     'match_method': 'Fuzzy Title (Score: 100)'},\n",
      " 'farshin_make_2019_pub': {'DiVA_ID': 'diva2:1291291',\n",
      "                           'match_method': 'Identifier (doi)'},\n",
      " 'ioannidis_coherent_1991_pub': {'DiVA_ID': 'diva2:461420',\n",
      "                                 'match_method': 'Identifier (doi)'},\n",
      " 'kim_small-mass_2016_pub': {'DiVA_ID': 'diva2:948742',\n",
      "                             'match_method': 'Identifier (doi)'},\n",
      " 'maguire_jr_new_2014_pub': {'DiVA_ID': 'diva2:690828',\n",
      "                             'match_method': 'Identifier (doi)'},\n",
      " 'verardo2023fmmheadenhancingautoencoderbasedecg_pub': {'DiVA_ID': 'diva2:1944107',\n",
      "                                                        'match_method': 'Fuzzy '\n",
      "                                                                        'Title '\n",
      "                                                                        '(Score: '\n",
      "                                                                        '99)'}}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. More Robust Normalization Function ---\n",
    "def normalize_identifier(identifier):\n",
    "    \"\"\"Strips common prefixes and handles empty strings.\"\"\"\n",
    "    if not isinstance(identifier, str) or identifier.strip() == \"\":\n",
    "        return None # Return None for empty or non-string data\n",
    "    identifier = re.sub(r'^(https?://)?(doi.org/)?(doi:)?', '', identifier, flags=re.IGNORECASE)\n",
    "    return identifier.lower().strip()\n",
    "\n",
    "def normalize_title(title, subtitle=None, delimiter_pattern=r'[:—-]' ):\n",
    "    if pd.notna(subtitle):\n",
    "        full_title = f\"{title} {subtitle}\"\n",
    "    else:\n",
    "        full_title = str(title)\n",
    "    \n",
    "    parts = re.split(delimiter_pattern, full_title, 1)\n",
    "    cleaned_parts = [re.sub(r'[^\\w\\s]', '', part).lower().strip() for part in parts]\n",
    "    \n",
    "    return ' '.join(cleaned_parts)\n",
    "\n",
    "# --- 2. Pre-process the DataFrame ---\n",
    "identifier_cols = ['doi'] # Add 'url', 'isbn', etc. as needed\n",
    "def preprocess_dataframe(df_to_process):\n",
    "    \"\"\"\n",
    "    Adds normalized columns to the DataFrame for efficient searching.\n",
    "    \"\"\"\n",
    "    # *** THE CRITICAL FIX ***\n",
    "    # Create an explicit copy to work on. This prevents the SettingWithCopyWarning.\n",
    "    df_processed = df_to_process.copy()\n",
    "\n",
    "    # List of identifier columns to check\n",
    "    identifier_cols = ['doi', 'url', 'isbn', 'pmid']\n",
    "    for col in identifier_cols:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[f'norm_{col}'] = df_processed[col].apply(normalize_identifier)\n",
    "\n",
    "    # List of title/subtitle language pairs to check\n",
    "    title_cols = {\n",
    "        'eng': ('title.eng', 'subtitle.eng'),\n",
    "        'swe': ('title.swe', 'subtitle.swe')\n",
    "    }\n",
    "    for lang, (title_col, sub_col) in title_cols.items():\n",
    "        if title_col in df_processed.columns:\n",
    "            if sub_col in df_processed.columns:\n",
    "                df_processed[f'norm_title_{lang}'] = df_processed.apply(lambda row: normalize_title(row[title_col], row[sub_col]), axis=1)\n",
    "            else:\n",
    "                df_processed[f'norm_title_{lang}'] = df_processed[title_col].apply(normalize_title)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "    \n",
    "# --- 3. Main Matching Logic with Debugging ---\n",
    "def find_diva_ids(bib_database, dataframe):\n",
    "    #parser = BibTexParser(ignore_nonstandard_types=False)\n",
    "    #bib_database = bibtexparser.loads(bibtex_content, parser=parser)\n",
    "    results = {}\n",
    "    FUZZY_MATCH_THRESHOLD = 90\n",
    "    print(\"--- Starting Match Process ---\\n\")\n",
    "    \n",
    "    for entry in bib_database.entries:\n",
    "        bib_key = entry['ID']\n",
    "        print(f\"Processing BibTeX key: {bib_key}\")\n",
    "        found_match = None\n",
    "\n",
    "        # Method 1: Match by Identifiers\n",
    "        for id_type in identifier_cols:\n",
    "            if id_type in entry and f'norm_{id_type}' in dataframe.columns:\n",
    "                norm_bib_id = normalize_identifier(entry[id_type])\n",
    "                \n",
    "                # --- DEBUGGING PRINTS ---\n",
    "                print(f\"  Attempting to match on '{id_type}'...\")\n",
    "                print(f\"    - Raw .bib value:      '{entry.get(id_type)}'\")\n",
    "                print(f\"    - Normalized .bib value: '{norm_bib_id}'\")\n",
    "                \n",
    "                if norm_bib_id:\n",
    "                    id_match = dataframe[dataframe[f'norm_{id_type}'] == norm_bib_id]\n",
    "                    if not id_match.empty:\n",
    "                        diva_id = id_match.iloc[0]['recordInfo.recordIdentifier']\n",
    "                        found_match = (diva_id, f\"Identifier ({id_type})\")\n",
    "                        print(f\"    -> SUCCESS: Found DiVA ID {diva_id}\")\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"    -> SKIPPED: Identifier is empty after normalization.\")\n",
    "        if found_match:\n",
    "            results[bib_key] = {'DiVA_ID': found_match[0], 'match_method': found_match[1]}\n",
    "            continue\n",
    "\n",
    "        # Method 2: Fallback to Fuzzy Title Matching\n",
    "        if 'title' in entry:\n",
    "            norm_bib_title = normalize_title(entry['title'])\n",
    "            # print(f\"{norm_bib_title=}\")\n",
    "            best_score = 0\n",
    "            best_match_id = None\n",
    "            \n",
    "            title_cols_map = {\n",
    "                'eng': ('title.eng', 'subtitle.eng'),\n",
    "                'swe': ('title.swe', 'subtitle.swe')\n",
    "            }\n",
    "            \n",
    "            for lang in title_cols_map.keys():\n",
    "                if f'norm_title_{lang}' in dataframe.columns:\n",
    "                    scores = dataframe[f'norm_title_{lang}'].apply(lambda df_title: fuzz.ratio(norm_bib_title, df_title))\n",
    "                    if scores.max() > best_score:\n",
    "                        best_score = scores.max()\n",
    "                        best_match_id = dataframe.loc[scores.idxmax()]['recordInfo.recordIdentifier']\n",
    "\n",
    "            if best_score > FUZZY_MATCH_THRESHOLD:\n",
    "                found_match = (best_match_id, f\"Fuzzy Title (Score: {best_score})\")\n",
    "        if found_match:\n",
    "            results[bib_key] = {'DiVA_ID': found_match[0], 'match_method': found_match[1]}\n",
    "            continue\n",
    "\n",
    "        # method 3 - brute force check for matching software link\n",
    "        bib_type = entry['ENTRYTYPE']\n",
    "        if bib_type == 'software':\n",
    "            for idx, row in dataframe.iterrows():\n",
    "                #print(f\"{idx=}\")\n",
    "                diva_id=row['recordInfo.recordIdentifier']\n",
    "                d=row['Year']\n",
    "                bib_other_links=row['location.other_links']\n",
    "                bib_url=entry['url']\n",
    "                # 'url': 'https://doi.org/10.5281/zenodo.4435970'\n",
    "                doi_prefix='https://doi.org/'\n",
    "                if bib_url.startswith(doi_prefix):\n",
    "                    bib_pseudo_doi=bib_url[len(doi_prefix):]\n",
    "                # diva2:1527198: ['Software url: https://dl.acm.org/do/10.5281/zenodo.4435970/abs/', 'fulltext:postprint url: https://kth.diva-portal.org/smash/get/diva2:1527198/FULLTEXT01.pdf', 'fulltext:postprint url: https://kth.diva-portal.org/smash/get/diva2:1527198/FULLTEXT02.pdf']\n",
    "                if diva_id == 'diva2:1527198':\n",
    "                    #print(f\"{diva_id}: {bib_other_links} {type(bib_other_links)} {bib_pseudo_doi=}\")\n",
    "                    if isinstance(bib_other_links, list):\n",
    "                        for ol in bib_other_links:\n",
    "                            #print(f\"{ol=} {type(ol)=}\")\n",
    "                            software_url_prefix='Software url: '\n",
    "                            if ol.startswith(software_url_prefix):\n",
    "                                if ol[len(software_url_prefix):].find(bib_pseudo_doi) > 0:\n",
    "                                    found_match = (diva_id, f\"Software link (url: {ol[len(software_url_prefix):]})\")\n",
    "                                    break\n",
    "\n",
    "        \n",
    "        if found_match:\n",
    "            results[bib_key] = {'DiVA_ID': found_match[0], 'match_method': found_match[1]}\n",
    "        else:\n",
    "            print(\"  -> FAILED: No match found for this entry.\")\n",
    "            results[bib_key] = {'DiVA_ID': 'Not found in database', 'match_method': 'None'}\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- 4. Run and print final results ---\n",
    "\n",
    "df1=user_df.copy()\n",
    "df1=preprocess_dataframe(df1)\n",
    "found_entries = find_diva_ids(bib_database, df1)\n",
    "\n",
    "print(\"\\n--- Final Matching Results ---\")\n",
    "pprint(found_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication:\tDiVA Id\tBibliographic key\n",
      "1:\tdiva2:461420\tioannidis_coherent_1991_pub \n",
      "2:\tdiva2:690828\tmaguire_jr_new_2014_pub \n",
      "3:\tdiva2:1291291\tfarshin_make_2019_pub \n",
      "4:\tdiva2:948742\tkim_small-mass_2016_pub \n",
      "5:\tdiva2:1944107\tverardo2023fmmheadenhancingautoencoderbasedecg_pub \n",
      "6:\tdiva2:456465\tUS7107453B2_pub \n",
      "7:\tdiva2:1926019\tUS12111768B2_pub \n",
      "8:\tdiva2:1527198\t10.5281/zenodo.4435970_pub \n"
     ]
    }
   ],
   "source": [
    "def included_pubs_DiVA_ids(bib_database, found_entries):\n",
    "    print(f\"Publication:\\tDiVA Id\\tBibliographic key\")\n",
    "    for idx, entry in enumerate(bib_database.entries):\n",
    "        bib_key = entry['ID']\n",
    "        fe=found_entries[bib_key]\n",
    "        did=fe.get('DiVA_ID', None)\n",
    "        print(f\"{idx+1}:\\t{did}\\t{bib_key} \")\n",
    "\n",
    "included_pubs_DiVA_ids(bib_database, found_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "288156dbef364610b068b4d88c886f95",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
