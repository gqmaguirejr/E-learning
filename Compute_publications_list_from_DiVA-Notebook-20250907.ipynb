{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the list of publications for a third-cycle thesis based on student's publications in DiVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some installs of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: bibtexparser in /home/maguire/.local/lib/python3.11/site-packages (1.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in /home/maguire/.local/lib/python3.11/site-packages (from bibtexparser) (2.4.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: thefuzz in /home/maguire/.local/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: python-Levenshtein in /home/maguire/.local/lib/python3.11/site-packages (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /home/maguire/.local/lib/python3.11/site-packages (from thefuzz) (3.13.0)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in /home/maguire/.local/lib/python3.11/site-packages (from python-Levenshtein) (0.27.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bibtexparser\n",
    "!pip install thefuzz python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\t# to make OS calls, here to get time zone info\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import urllib.request\n",
    "import requests\n",
    "\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "from io import StringIO\n",
    "\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# import logging\n",
    "\n",
    "import pymods\n",
    "\n",
    "import bibtexparser\n",
    "from bibtexparser.bparser import BibTexParser\n",
    "\n",
    "\n",
    "\n",
    "from thefuzz import fuzz\n",
    "\n",
    "#from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, set the flag to True\n",
    "Verbose_Flag=False\n",
    "\n",
    "# set directory to use for output of the collected words\n",
    "directory_prefix ='/tmp/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4240e5585c844155a968494c43c5f171",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Define some helper functions and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "48c76af5b22d4468b0b206bc0cdd411a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1667202629310,
    "source_hash": "955ec4c5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_file_older_than_x_days(file, days=1): \n",
    "    file_time = os.path.getmtime(file) \n",
    "    # Check against 24 hours \n",
    "    return ((time.time() - file_time) / 3600 > 24*days)\n",
    "\n",
    "# define a function to compute lower case Roman numeras from an integer\n",
    "def int_to_roman(num):\n",
    "    res = \"\"\n",
    "    table = [\n",
    "        (1000, \"m\"),\n",
    "        (900, \"cm\"),\n",
    "        (500, \"d\"),\n",
    "        (400, \"cd\"),\n",
    "        (100, \"c\"),\n",
    "        (90, \"xc\"),\n",
    "        (50, \"l\"),\n",
    "        (40, \"xl\"),\n",
    "        (10, \"x\"),\n",
    "        (9, \"ix\"),\n",
    "        (5, \"v\"),\n",
    "        (4, \"iv\"),\n",
    "        (1, \"i\"),\n",
    "    ]\n",
    "    for cap, roman in table:\n",
    "        d, m = divmod(num, cap)\n",
    "        res += roman * d\n",
    "        num = m\n",
    "    return res\n",
    "\n",
    "# Using the above function fill a dict with the lower case Roman numerals from 0 to 499\n",
    "# These are precomputed, so later we can simply do a lookup of the string and et the integer.\n",
    "roman_table=dict()\n",
    "for i in range(0,500):\n",
    "    roman_table[int_to_roman(i)]=i\n",
    "\n",
    "def collect_originInfo(mod_elem):\n",
    "    originInfo=dict()\n",
    "    for elem in mod_elem:\n",
    "        if elem.tag.count(\"}languageTerm\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"LanguageTerm\"]=elem.text\n",
    "        elif elem.tag.count(\"}dateIssued\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"CreatedDate\"]=elem.text\n",
    "        elif elem.tag.count(\"}dateOther\") == 1:\n",
    "            # <dateOther type=\"defence\">2018-07-26T13:00:00</dateOther>\n",
    "            # <dateOther type=\"academicTerm\">VT 2018</dateOther>\n",
    "            # <dateOther type=\"availableFrom\">2018-11-19T09:16:45</dateOther>\n",
    "            if elem.text is not None:\n",
    "                type=elem.attrib.get('type')\n",
    "                if type == 'defence':\n",
    "                    originInfo['DefenceDate']=elem.text\n",
    "                elif type == 'academicTerm':\n",
    "                    originInfo['academicTerm']=elem.text\n",
    "                elif type == 'availableFrom':\n",
    "                    originInfo['PublicationDate']=elem.text\n",
    "                else:\n",
    "                    originInfo[\"dateOther\"]=elem.text\n",
    "        elif elem.tag.count(\"}place\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"place\"]=elem.text\n",
    "        elif elem.tag.count(\"}publisher\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"publisher\"]=elem.text\n",
    "        elif elem.tag.count(\"}edition\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"edition\"]=elem.text\n",
    "        elif elem.tag.count(\"}genre\") == 1:\n",
    "            if elem.text is not None:\n",
    "                originInfo[\"genre\"]=elem.text\n",
    "        else:\n",
    "            print(\"Unhandled case in collect_originInfo: {}\".format(elem))\n",
    "    return originInfo\n",
    "\n",
    "def update_key_list(diva_entry, key, name_struct):\n",
    "    current_value=diva_entry.get(key, list())\n",
    "    current_value.append(name_struct)\n",
    "    return current_value\n",
    "\n",
    "# The following have been adapted from https://www.loc.gov/marc/relators/relaterm.html\n",
    "# for use in the content of KTH, where a 'mon' is an 'examiner' and a 'ths' is a 'supervisor'\n",
    "\n",
    "MARC_Code_for_Relators = {\n",
    "    'abr': 'abridger',\n",
    "    'acp': 'art copyist',\n",
    "    'act': 'actor',\n",
    "    'adi': 'art director',\n",
    "    'adp': 'adapter',\n",
    "    'anl': 'analyst',\n",
    "    'anm': 'animator',\n",
    "    'ann': 'annotator',\n",
    "    'anc': 'announcer',\n",
    "    'ant': 'bibliographic antecedent',\n",
    "    'ape': 'appellee',\n",
    "    'apl': 'appellant',\n",
    "    'app': 'applicant',\n",
    "    'aqt': 'author in quotations or text abstracts',\n",
    "    'arc': 'architect',\n",
    "    'ard': 'artistic director',\n",
    "    'arr': 'arranger',\n",
    "    'art': 'artist',\n",
    "    'asg': 'assignee',\n",
    "    'asn': 'associated name',\n",
    "    'ato': 'autographer',\n",
    "    'att': 'attributed name',\n",
    "    'auc': 'auctioneer',\n",
    "    'aud': 'author of dialog',\n",
    "    'aue': 'audio engineer',\n",
    "    'aui': 'author of introduction',\n",
    "    'aup': 'audio producer',\n",
    "    'aus': 'screenwriter',\n",
    "    'aut': 'author',\n",
    "    'bdd': 'binding designer',\n",
    "    'bjd': 'bookjacket designer',\n",
    "    'bka': 'book artist',\n",
    "    'bkd': 'book designer',\n",
    "    'bkp': 'book producer',\n",
    "    'blw': 'blurb writer',\n",
    "    'bnd': 'binder',\n",
    "    'bpd': 'bookplate designer',\n",
    "    'brd': 'broadcaster',\n",
    "    'brl': 'braille embosser',\n",
    "    'bsl': 'bookseller',\n",
    "    'cad': 'casting director',\n",
    "    'cas': 'caster',\n",
    "    'ccp': 'conceptor',\n",
    "    'chr': 'choreographer',\n",
    "    'cli': 'client',\n",
    "    'cll': 'calligrapher',\n",
    "    'clr': 'colorist',\n",
    "    'clt': 'collotyper',\n",
    "    'cmm': 'commentator',\n",
    "    'cmp': 'composer',\n",
    "    'cmt': 'compositor',\n",
    "    'cnd': 'conductor',\n",
    "    'cng': 'cinematographer',\n",
    "    'cns': 'censor',\n",
    "    'coe': 'contestant-appellee',\n",
    "    'col': 'collector',\n",
    "    'com': 'compiler',\n",
    "    'con': 'conservator',\n",
    "    'cop': 'camera operator',\n",
    "    'cor': 'collection registrar',\n",
    "    'cos': 'contestant',\n",
    "    'cot': 'contestant-appellant',\n",
    "    'cou': 'court governed',\n",
    "    'cov': 'cover designer',\n",
    "    'cpc': 'copyright claimant',\n",
    "    'cpe': 'complainant-appellee',\n",
    "    'cph': 'copyright holder',\n",
    "    'cpl': 'complainant',\n",
    "    'cpt': 'complainant-appellant',\n",
    "    'cre': 'creator',\n",
    "    'crp': 'correspondent',\n",
    "    'crr': 'corrector',\n",
    "    'crt': 'court reporter',\n",
    "    'csl': 'consultant',\n",
    "    'csp': 'consultant to a project',\n",
    "    'cst': 'costume designer',\n",
    "    'ctb': 'contributor',\n",
    "    'cte': 'contestee-appellee',\n",
    "    'ctg': 'cartographer',\n",
    "    'ctr': 'contractor',\n",
    "    'cts': 'contestee',\n",
    "    'ctt': 'contestee-appellant',\n",
    "    'cur': 'curator',\n",
    "    'cwt': 'commentator for written text',\n",
    "    'dbd': 'dubbing director',\n",
    "    'dbp': 'distribution place',\n",
    "    'dfd': 'defendant',\n",
    "    'dfe': 'defendant-appellee',\n",
    "    'dft': 'defendant-appellant',\n",
    "    'dgc': 'degree committee member',\n",
    "    'dgg': 'degree granting institution',\n",
    "    'dgs': 'degree supervisor',\n",
    "    'dis': 'dissertant',\n",
    "    'djo': 'dj',\n",
    "    'dln': 'delineator',\n",
    "    'dnc': 'dancer',\n",
    "    'dnr': 'donor',\n",
    "    'dpc': 'depicted',\n",
    "    'dpt': 'depositor',\n",
    "    'drm': 'draftsman',\n",
    "    'drt': 'director',\n",
    "    'dsr': 'designer',\n",
    "    'dst': 'distributor',\n",
    "    'dtc': 'data contributor',\n",
    "    'dte': 'dedicatee',\n",
    "    'dtm': 'data manager',\n",
    "    'dto': 'dedicator',\n",
    "    'dub': 'dubious author',\n",
    "    'edc': 'editor of compilation',\n",
    "    'edd': 'editorial director',\n",
    "    'edm': 'editor of moving image work',\n",
    "    'edt': 'editor',\n",
    "    'egr': 'engraver',\n",
    "    'elg': 'electrician',\n",
    "    'elt': 'electrotyper',\n",
    "    'enj': 'enacting jurisdiction',\n",
    "    'eng': 'engineer',\n",
    "    'etr': 'etcher',\n",
    "    'evp': 'event place',\n",
    "    'exp': 'expert',\n",
    "    'fac': 'facsimilist',\n",
    "    'fds': 'film distributor',\n",
    "    'fld': 'field director',\n",
    "    'flm': 'film editor',\n",
    "    'fmd': 'film director',\n",
    "    'fmk': 'filmmaker',\n",
    "    'fmo': 'former owner',\n",
    "    'fmp': 'film producer',\n",
    "    'fnd': 'funder',\n",
    "    'fon': 'founder',\n",
    "    'fpy': 'first party',\n",
    "    'frg': 'forger',\n",
    "    'gdv': 'game developer',\n",
    "    'gis': 'geographic information specialist',\n",
    "    'his': 'host institution',\n",
    "    'hnr': 'honoree',\n",
    "    'hst': 'host',\n",
    "    'ill': 'illustrator',\n",
    "    'ilu': 'illuminator',\n",
    "    'ink': 'inker',\n",
    "    'ins': 'inscriber',\n",
    "    'inv': 'inventor',\n",
    "    'isb': 'issuing body',\n",
    "    'itr': 'instrumentalist',\n",
    "    'ive': 'interviewee',\n",
    "    'ivr': 'interviewer',\n",
    "    'jud': 'judge',\n",
    "    'jug': 'jurisdiction governed',\n",
    "    'lbr': 'laboratory',\n",
    "    'lbt': 'librettist',\n",
    "    'ldr': 'laboratory director',\n",
    "    'led': 'lead',\n",
    "    'lee': 'libelee-appellee',\n",
    "    'lel': 'libelee',\n",
    "    'len': 'lender',\n",
    "    'let': 'libelee-appellant',\n",
    "    'lgd': 'lighting designer',\n",
    "    'lie': 'libelant-appellee',\n",
    "    'lil': 'libelant',\n",
    "    'lit': 'libelant-appellant',\n",
    "    'lsa': 'landscape architect',\n",
    "    'lse': 'licensee',\n",
    "    'lso': 'licensor',\n",
    "    'ltg': 'lithographer',\n",
    "    'ltr': 'letterer',\n",
    "    'lyr': 'lyricist',\n",
    "    'mcp': 'music copyist',\n",
    "    'mdc': 'metadata contact',\n",
    "    'med': 'medium',\n",
    "    'mfp': 'manufacture place',\n",
    "    'mfr': 'manufacturer',\n",
    "    'mka': 'makeup artist',\n",
    "    'mod': 'moderator',\n",
    "    #'mon': 'monitor',\n",
    "    'mon': 'examiner',\n",
    "    'mrb': 'marbler',\n",
    "    'mrk': 'markup editor',\n",
    "    'msd': 'musical director',\n",
    "    'mte': 'metal-engraver',\n",
    "    'mtk': 'minute taker',\n",
    "    'mup': 'music programmer',\n",
    "    'mus': 'musician',\n",
    "    'mxe': 'mixing engineer',\n",
    "    'nan': 'news anchor',\n",
    "    'nrt': 'narrator',\n",
    "    'onp': 'onscreen participant',\n",
    "    'opn': 'opponent',\n",
    "    'osp': 'onscreen presenter',\n",
    "    'org': 'organizer',\n",
    "    'orm': 'organizer',\n",
    "    'oth': 'other',\n",
    "    'own': 'owner',\n",
    "    'pad': 'place of address',\n",
    "    'pan': 'panelist',\n",
    "    'pat': 'patron',\n",
    "    'pbd': 'publishing director',\n",
    "    'pbl': 'publisher',\n",
    "    'pdr': 'project director',\n",
    "    'pfr': 'proofreader',\n",
    "    'pgr': 'programmer',\n",
    "    'pht': 'photographer',\n",
    "    'plt': 'platemaker',\n",
    "    'pma': 'permitting agency',\n",
    "    'pmn': 'production manager',\n",
    "    'pnc': 'penciller',\n",
    "    'pop': 'printer of plates',\n",
    "    'ppm': 'papermaker',\n",
    "    'ppt': 'puppeteer',\n",
    "    'pra': 'praeses',\n",
    "    'prc': 'process contact',\n",
    "    'prd': 'production personnel',\n",
    "    'pre': 'presenter',\n",
    "    'prf': 'performer',\n",
    "    'prg': 'programmer',\n",
    "    'prm': 'printmaker',\n",
    "    'prn': 'production company',\n",
    "    'pro': 'producer',\n",
    "    'prp': 'production place',\n",
    "    'prs': 'production designer',\n",
    "    'prt': 'printer',\n",
    "    'prv': 'provider',\n",
    "    'pta': 'patent applicant',\n",
    "    'pte': 'plaintiff-appellee',\n",
    "    'pth': 'patent holder',\n",
    "    'ptf': 'plaintiff',\n",
    "    'ptt': 'plaintiff-appellant',\n",
    "    'pup': 'publication place',\n",
    "    'rbr': 'rubricator',\n",
    "    'rcd': 'recordist',\n",
    "    'rce': 'recording engineer',\n",
    "    'rcp': 'addressee',\n",
    "    'rdd': 'radio director',\n",
    "    'red': 'redaktor',\n",
    "    'ren': 'renderer',\n",
    "    'res': 'researcher',\n",
    "    'rev': 'reviewer',\n",
    "    'rpc': 'radio producer',\n",
    "    'rps': 'repository',\n",
    "    'rpt': 'reporter',\n",
    "    'rpy': 'responsible party',\n",
    "    'rsd': 'stage director',\n",
    "    'rsg': 'restager',\n",
    "    'rsr': 'restorationist',\n",
    "    'rsp': 'respondent',\n",
    "    'rst': 'respondent-appellant',\n",
    "    'rse': 'respondent-appellee',\n",
    "    'rth': 'research team head',\n",
    "    'rtm': 'research team member',\n",
    "    'rxa': 'remix artist',\n",
    "    'sad': 'scientific advisor',\n",
    "    'sce': 'scenarist',\n",
    "    'scl': 'sculptor',\n",
    "    'scr': 'scribe',\n",
    "    'sds': 'sound designer',\n",
    "    'sde': 'sound engineer',\n",
    "    'sec': 'secretary',\n",
    "    'sfx': 'special effects provider',\n",
    "    'sgd': 'stage director',\n",
    "    'sgn': 'signer',\n",
    "    'sht': 'supporting host',\n",
    "    'sll': 'seller',\n",
    "    'sng': 'singer',\n",
    "    'spk': 'speaker',\n",
    "    'spn': 'sponsor',\n",
    "    'spy': 'second party',\n",
    "    'srv': 'surveyor',\n",
    "    'std': 'set designer',\n",
    "    'stg': 'setting',\n",
    "    'stm': 'stage manager',\n",
    "    'stn': 'standards body',\n",
    "    'str': 'stereotyper',\n",
    "    'stl': 'storyteller',\n",
    "    'swd': 'software developer',\n",
    "    'tad': 'technical advisor',\n",
    "    'tau': 'television writer',\n",
    "    'tcd': 'technical director',\n",
    "    'tch': 'teacher',\n",
    "    #'ths': 'thesis advisor',\n",
    "    'ths': 'supervisor',\n",
    "    'tld': 'television director',\n",
    "    'tlg': 'television guest',\n",
    "    'tlh': 'television host',\n",
    "    'tlp': 'television producer',\n",
    "    'trc': 'transcriber',\n",
    "    'trl': 'translator',\n",
    "    'tyd': 'type designer',\n",
    "    'tyg': 'typographer',\n",
    "    'uvp': 'university place',\n",
    "    'vac': 'voice actor',\n",
    "    'vdg': 'videographer',\n",
    "    'vfx': 'visual effects provider',\n",
    "    'wac': 'writer of added commentary',\n",
    "    'wal': 'writer of added lyrics',\n",
    "    'wam': 'writer of accompanying material',\n",
    "    'wat': 'writer of added text',\n",
    "    'waw': 'writer of afterword',\n",
    "    'wdc': 'woodcutter',\n",
    "    'wde': 'wood engraver',\n",
    "    'wfs': 'writer of film story',\n",
    "    'wfw': 'writer of foreword',\n",
    "    'wft': 'writer of intertitles',\n",
    "    'win': 'writer of introduction',\n",
    "    'wit': 'witness',\n",
    "    'wpr': 'writer of preface',\n",
    "    'wst': 'writer of supplementary textual content',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to parse the MODS record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbosePrint(msg):\n",
    "    global Verbose_Flag\n",
    "    if Verbose_Flag:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def mods_records_to_dataframe(mods_records): # return a dataframe\n",
    "    n_index=0                                # index over entries\n",
    "    df=pd.DataFrame()           # define a dataframe to collect the results\n",
    "    df2=pd.DataFrame()          # define a dataframe for each entry\n",
    "    for record in mods_records:\n",
    "        n_index=n_index+1\n",
    "        diva_entry=dict()\n",
    "        diva_entry['node']=n_index\n",
    "        verbosePrint(f\"{n_index=} {record=}\")\n",
    "        verbosePrint(record.tag)\n",
    "        if record.tag.count(\"}mods\") == 1:\n",
    "            #print(\"Attribute: {}\".format(record.attrib))\n",
    "            for mod_element in record:\n",
    "                verbosePrint(mod_element)\n",
    "                if mod_element.tag.count(\"}genre\") >= 1:\n",
    "                    if mod_element.attrib.get('authority') == \"diva\" and mod_element.attrib.get('type') == \"publicationTypeCode\":\n",
    "                        diva_entry['publicationTypeCode']=mod_element.text\n",
    "                        verbosePrint(F\"publicationTypeCode= {mod_element.text}\")\n",
    "                        if mod_element.attrib.get('type') == \"publicationType\":\n",
    "                            authority=mod_element.attrib.get('authority')\n",
    "                            current_pubtype=diva_entry.get('publicationType', dict())\n",
    "                            if authority == 'diva':\n",
    "                                lang=mod_element.attrib.get('lang')\n",
    "                                if lang:\n",
    "                                    current_pubtype.update({lang: mod_element.text})\n",
    "                                diva_entry['diva_publicationType']=current_pubtype\n",
    "                            elif authority == 'svep':\n",
    "                                diva_entry['svep_publicationType']=mod_element.text\n",
    "                            elif authority == 'kev':\n",
    "                                lang=mod_element.attrib.get('lang')\n",
    "                                current_pubtype=diva_entry.get('kev_publicationType', dict())\n",
    "                                if lang:\n",
    "                                    current_pubtype.update({lang: mod_element.text})\n",
    "                                diva_entry['kev_publicationType']=current_pubtype\n",
    "                            else:\n",
    "                                print(f\"Unhandled case in publicationType: {mod_element.attrib=} {mod_element.text=}\")\n",
    "                elif mod_element.tag.count(\"}name\") == 1:\n",
    "                    # <name type=\"personal\" authority=\"kth\" xlink:href=\"u19gy7zg\">\n",
    "                    # <name type=\"corporate\" authority=\"kth\" xlink:href=\"5956\"><namePart>KTH</namePart><namePart>Skolan för datavetenskap och kommunikation (CSC)</namePart>\n",
    "                    if mod_element.attrib.get('type') == \"personal\":\n",
    "                        name_type='personal'\n",
    "                    elif mod_element.attrib.get('type') == \"corporate\":\n",
    "                        name_type='corporate'\n",
    "                    elif mod_element.attrib.get('type') == \"conference\":\n",
    "                        name_type='conference'    \n",
    "                    else:\n",
    "                        name_type='unknown'\n",
    "                    name_struct={'type': name_type}\n",
    "                    name_authority=mod_element.attrib.get('authority')\n",
    "                    if name_authority is not None:\n",
    "                        name_struct['authority']=name_authority\n",
    "                    xlink=mod_element.attrib.get('{http://www.w3.org/1999/xlink}href', None)\n",
    "                    if xlink is not None:\n",
    "                        name_struct['xlink']=xlink\n",
    "                    for elem in mod_element:\n",
    "                        if elem.tag.count(\"}namePart\") == 1:\n",
    "                            # personal name\n",
    "                            namePart_type=elem.attrib.get('type')\n",
    "                            if namePart_type == 'family':\n",
    "                                name_struct['family']=elem.text\n",
    "                            elif namePart_type == 'given':\n",
    "                                name_struct['given']=elem.text\n",
    "                            elif namePart_type == 'termsOfAddress':\n",
    "                                # <namePart type=\"termsOfAddress\">professor</namePart>\n",
    "                                name_struct['termsOfAddress']=elem.text\n",
    "                            elif name_type =='conference':\n",
    "                                name_struct['conference']=elem.text\n",
    "                            else:\n",
    "                                orglevel=0\n",
    "                                orglevels=dict()\n",
    "                                for org in elem:\n",
    "                                    orglevelname=f\"L{orglevel}\"\n",
    "                                    orglevels[orglevelname]=elem.text\n",
    "                                    orglevel=orglevel+1\n",
    "                                if orglevels:\n",
    "                                    name_struct['orglevels']=orglevels\n",
    "                        elif elem.tag.count(\"}role\") == 1:\n",
    "                            # <role><roleTerm type=\"code\" authority=\"marcrelator\">pbl</roleTerm>\n",
    "                            for role in elem:\n",
    "                                role_type=role.attrib.get('type')\n",
    "                                role_authority=role.attrib.get('authority')\n",
    "                                # the codes come from https://www.loc.gov/marc/relators/relaterm.html\n",
    "                                if role_type=='code'and role_authority=='marcrelator':\n",
    "                                    if role.text in MARC_Code_for_Relators:\n",
    "                                        name_struct['role']=MARC_Code_for_Relators[role.text]\n",
    "                                    else:\n",
    "                                        print(f'Unhandled role: {role.text}')\n",
    "                        elif elem.tag.count(\"}affiliation\") == 1:\n",
    "                            # <affiliation>KTH, Kommunikationssystem, CoS</affiliation>\n",
    "                            name_struct['affiliation']=elem.text\n",
    "                        elif elem.tag.count(\"}description\") == 1:\n",
    "                            # <description>orcid.org=0000-0002-6066-746X</description>\n",
    "                            name_struct['description']=elem.text\n",
    "                        else:\n",
    "                            print('Unhandled case of role')\n",
    "                    name_role=name_struct.get('role', None)\n",
    "                    if name_role is not None:\n",
    "                        # There should only be one examiner, but we support several\n",
    "                        # There can be multiple supervisors and authors\n",
    "                        if name_role in ['examiner',  'supervisor', 'opponent',  'applicant', 'architect', 'author', 'author of dialog',\n",
    "                                         'author of introduction', 'author in quotations or text abstracts', 'editor', 'artist',\n",
    "                                         'cinematographer', 'commentator',\n",
    "                                         'commentator for written text', 'contributor', 'cover designer',\n",
    "                                         'creator', 'curator',  'designer', 'director', 'dissertant',\n",
    "                                         'filmmaker',  'illustrator', 'inventor',\n",
    "                                         'narrator', 'photographer', 'producer', 'project director', 'programmer',\n",
    "                                         'publisher', 'researcher', 'screenwriter', 'translator', 'writer of accompanying material'\n",
    "                                         ]:\n",
    "                            diva_entry[name_role] = update_key_list(diva_entry, name_role, name_struct)\n",
    "                        elif name_role == 'other':\n",
    "                            if name_struct.get('type', None) == 'corporate':\n",
    "                                if name_struct.get('description', None) == 'Research Group':\n",
    "                                    diva_entry['research group']= name_struct\n",
    "                                else:\n",
    "                                    print(f'Unhandled name - other: {name_struct} with role: {name_role}')\n",
    "                            else:\n",
    "                                print(f'Unhandled name - unknown: {name_struct} with role: {name_role}')\n",
    "                    else:\n",
    "                        if name_type == 'conference':\n",
    "                            diva_entry['conference']=name_struct.get('conference')\n",
    "                        elif name_type == 'personal': # as it an author but role not explicit\n",
    "                            diva_entry['author'] = update_key_list(diva_entry, 'author', name_struct)\n",
    "                        else:\n",
    "                            print(f'Unhandled name - with name_type: {name_type=} {name_struct=} {diva_entry=}')\n",
    "                elif mod_element.tag.count(\"}titleInfo\") == 1:\n",
    "                    #current_titleInfo=diva_entry.get('titleInfo', dict())\n",
    "                    #diva_entry['titleInfo']=current_titleInfo\n",
    "                    lang=mod_element.attrib.get('lang')\n",
    "                    for elem in mod_element:\n",
    "                        if elem.tag.count(\"}title\") == 1:\n",
    "                            #current_title=current_titleInfo.get('title', dict())\n",
    "                            current_title=diva_entry.get('title', dict())\n",
    "                            current_title.update({lang: elem.text})\n",
    "                            diva_entry['title']=current_title\n",
    "                        elif elem.tag.count(\"}subTitle\") == 1:\n",
    "                            #current_subtitle=current_titleInfo.get('subtitle', dict())\n",
    "                            current_subtitle=diva_entry.get('subtitle', dict())\n",
    "                            current_subtitle.update({lang: elem.text})\n",
    "                            diva_entry['subtitle']=current_subtitle\n",
    "                        else:\n",
    "                            print(\"Unhandled case in titleInfo\")\n",
    "                elif mod_element.tag.count(\"}language\") == 1:\n",
    "                    i=0\n",
    "                    temp_dict=dict()\n",
    "                    for elem in mod_element:\n",
    "                        i=i+1\n",
    "                        if elem.tag.count(\"}languageTerm\") == 1:\n",
    "                            if elem.text is not None:\n",
    "                                name='languageTerm_{0}'.format(i)\n",
    "                                temp_dict[name]=[elem.text]\n",
    "                        elif elem.tag.count(\"}dateIssued\") == 1:\n",
    "                            if elem.text is not None:\n",
    "                                verbosePrint(\"dateIssued: \".format(elem.text))\n",
    "                                name='dateIssued_{0}'.format(i)\n",
    "                                temp_dict[name]=[elem.text]\n",
    "                        elif elem.tag.count(\"}dateOther\") == 1:\n",
    "                            if elem.text is not None:\n",
    "                                verbosePrint(\"dateOther: {0}{1}\".format(elem.attrib, elem.text))\n",
    "                                name='dateOther_{0}'.format(i)\n",
    "                                temp_dict[name]=[elem.text]\n",
    "                        else:\n",
    "                            verbosePrint(\"mod_emem[\" + str(i) +\"]\".format(elem))\n",
    "                            name='language_{0}'.format(i)\n",
    "                            temp_dict[name]=[elem.text]\n",
    "                    verbosePrint(\"temp_dict={}\".format(temp_dict))\n",
    "                elif mod_element.tag.count(\"}originInfo\") == 1:\n",
    "                    diva_entry['originInfo']=collect_originInfo(mod_element)\n",
    "                    diva_entry['Year']=diva_entry['originInfo'].get(\"CreatedDate\", 'Unknown year')\n",
    "                elif mod_element.tag.count(\"}identifier\") == 1:\n",
    "                    if mod_element.text is not None and mod_element.attrib.get('type') in ['libris', 'articleId', 'url', 'doi', 'pmid', 'isi', 'scopus', 'uri', 'isrn', 'isbn', 'local']:\n",
    "                        diva_entry[mod_element.attrib.get('type')]=mod_element.text\n",
    "                    else:\n",
    "                        print(f\"unexpected identifier type: {mod_element.attrib.get('type')}\")\n",
    "                elif mod_element.tag.count(\"}abstract\") == 1:\n",
    "                    current_abstracts=diva_entry.get('abstract', dict())\n",
    "                    lang=mod_element.attrib.get('lang')\n",
    "                    if lang == '-1' or not isinstance(lang , str):\n",
    "                        # print(f\"{lang=} in {diva_entry=}\")\n",
    "                        lang='swe' # corect the entry in  diva2:905489\n",
    "                    current_abstracts.update({lang: mod_element.text})\n",
    "                    diva_entry['abstract']=current_abstracts\n",
    "                elif mod_element.tag.count(\"}subject\") == 1:\n",
    "                    #<subject lang=\"eng\" xlink:href=\"9895\"><topic>Master of Science - Computer Science</topic><genre>Educational program</genre></subject>\n",
    "                    #<subject lang=\"swe\" xlink:href=\"9895\"><topic>Teknologie masterexamen - Datalogi</topic><genre>Educational program</genre></subject>\n",
    "                    #<subject lang=\"eng\" xlink:href=\"10280\"><topic>Computer Science</topic><genre>Subject/course</genre></subject>\n",
    "                    #<subject lang=\"swe\" xlink:href=\"10280\"><topic>Datalogi</topic><genre>Subject/course</genre></subject>\n",
    "                    xlink=mod_element.attrib.get('{http://www.w3.org/1999/xlink}href', None)\n",
    "                    xlinks=mod_element.attrib.get('xlink', None)\n",
    "                    authority=mod_element.attrib.get('authority', None)\n",
    "                    if authority:\n",
    "                        subject=f'{authority}'\n",
    "                    elif xlink:    \n",
    "                        subject='xlink' \n",
    "                    elif xlinks:    \n",
    "                        subject='xlinks' \n",
    "                    else:\n",
    "                        subject='keywords'\n",
    "                    lang=mod_element.attrib.get('lang')\n",
    "                    current_subject=diva_entry.get(subject, dict())\n",
    "                    topics=current_subject.get(lang, list())\n",
    "                    for elem in mod_element:\n",
    "                        if elem.tag.count(\"}topic\") == 1:\n",
    "                            topics.append(elem.text)\n",
    "                            current_subject.update({lang: topics})\n",
    "                    diva_entry[subject]=current_subject\n",
    "                elif mod_element.tag.count(\"}recordInfo\") == 1:\n",
    "                    #<recordInfo>\n",
    "                    #<recordOrigin>u1d13i2c</recordOrigin>\n",
    "                    #<recordContentSource>kth</recordContentSource>\n",
    "                    #<recordCreationDate>2019-06-26</recordCreationDate>\n",
    "                    #<recordChangeDate>2022-06-26</recordChangeDate>\n",
    "                    #<recordIdentifier>diva2:1330685</recordIdentifier>\n",
    "                    #</recordInfo>\n",
    "                    current_recordInfo=diva_entry.get('recordInfo', dict())\n",
    "                    for elem in mod_element:\n",
    "                        #if elem.text is not None:\n",
    "                        #    print(f\"in recordInfo {elem.text}\")\n",
    "                        if elem.tag.count(\"}recordOrigin\") == 1:\n",
    "                            current_recordInfo[\"recordOrigin\"]=elem.text\n",
    "                        elif elem.tag.count(\"}recordContentSource\") == 1:\n",
    "                            current_recordInfo[\"recordContentSource\"]=elem.text\n",
    "                        elif elem.tag.count(\"}recordCreationDate\") == 1:\n",
    "                            current_recordInfo[\"recordCreationDate\"]=elem.text\n",
    "                        elif elem.tag.count(\"}recordChangeDate\") == 1:\n",
    "                            current_recordInfo[\"recordChangeDate\"]=elem.text\n",
    "                        elif elem.tag.count(\"}recordIdentifier\") == 1:\n",
    "                            current_recordInfo[\"recordIdentifier\"]=elem.text\n",
    "                        else:\n",
    "                            print(\"unhandled case in recordInfo\")\n",
    "                        diva_entry['recordInfo']=current_recordInfo\n",
    "                elif mod_element.tag.count(\"}location\") == 1:\n",
    "                    # <location><url displayLabel=\"fulltext\" note=\"free\" access=\"raw object\">http://kth.diva-portal.org/smash/get/diva2:821850/FULLTEXT01.pdf</url></location>\n",
    "                    current_location=diva_entry.get('location', dict())\n",
    "                    for elem in mod_element:\n",
    "                        if elem.tag.count(\"}url\") == 1:\n",
    "                            label=elem.attrib.get('displayLabel', None)\n",
    "                            if label:\n",
    "                                if elem.text:\n",
    "                                    if elem.text == diva_entry['title'].get('eng', None) or elem.text == diva_entry['title'].get('swe', None):\n",
    "                                        cl=current_location.get('url', list())\n",
    "                                        cl.append(elem.text)\n",
    "                                        current_location['url']=cl\n",
    "                                    elif label.find(',') >= 0 or label.startswith('Betydelsen av skuggning'):\n",
    "                                        cl=current_location.get('url', list())\n",
    "                                        cl.append(elem.text)\n",
    "                                        current_location['url']=cl\n",
    "                                    elif label == 'Fulltext' or label == 'fulltext' or label == 'Kandidatexjobb i Medieteknik (DM129X) år 2010':\n",
    "                                        cl=current_location.get('url', list())\n",
    "                                        cl.append(elem.text)\n",
    "                                        current_location['url']=cl\n",
    "                                    else:\n",
    "                                        cl=current_location.get('other_links', list())\n",
    "                                        cl.append(label+' url: '+elem.text)\n",
    "                                        current_location['other_links']=cl\n",
    "                                else:\n",
    "                                    cl=current_location.get(label, list())\n",
    "                                    cl.append(\"\")\n",
    "                                    current_location[label]=cl\n",
    "                            else:\n",
    "                                if elem.text:\n",
    "                                    cl=current_location.get('url', list())\n",
    "                                    cl.append(elem.text)\n",
    "                                    current_location['url']=cl\n",
    "                                else:\n",
    "                                    cl=current_location.get('url', list())\n",
    "                                    cl.append(\"\")\n",
    "                                    current_location['url']=cl\n",
    "                        else:\n",
    "                            print(\"Unhandled case in }location\")\n",
    "                        diva_entry['location']=current_location\n",
    "                elif mod_element.tag.count(\"}typeOfResource\") == 1:\n",
    "                    # <typeOfResource>text</typeOfResource>\n",
    "                    diva_entry['typeOfResource']=mod_element.text\n",
    "                elif mod_element.tag.count(\"}relatedItem\") == 1:            \n",
    "                    #<relatedItem type=\"series\">\n",
    "                    #  <titleInfo><title>Trita-ICT-COS</title></titleInfo>\n",
    "                    #  <identifier type=\"issn\">1653-6347</identifier>\n",
    "                    #  <identifier type=\"local\">247</identifier>\n",
    "                    #  <identifier type=\"issue number\">COS/CCS 2007-24</identifier>\n",
    "                    #</relatedItem>\n",
    "                    relatedItemType=mod_element.attrib.get('type', None)\n",
    "                    if not relatedItemType:\n",
    "                        relatedItemType='relatedItem'\n",
    "                    titleInfo=dict()\n",
    "                    for elem in mod_element:\n",
    "                        if relatedItemType == 'host':\n",
    "                            titleInfo[\"host\"]='host'\n",
    "                        if elem is not None:\n",
    "                            if elem.tag.count(\"}titleInfo\") == 1:\n",
    "                                for title in elem:\n",
    "                                    if title is not None and title.text:\n",
    "                                        titleInfo[\"title\"]=title.text\n",
    "                            elif elem.tag.count(\"}identifier\") >= 1:\n",
    "                                for subelem in elem:\n",
    "                                    identifier_type=subelem.attrib.get('type', None)\n",
    "                                    titleInfo[f\"{identifier_type}\"]=subelem.text\n",
    "                            elif elem.tag.count(\"}note\") == 1:\n",
    "                                note_type=elem.attrib.get('type', None)\n",
    "                                if note_type is None:\n",
    "                                    note_type='note'\n",
    "                                if elem.text is not None:\n",
    "                                    titleInfo[note_type]=elem.text\n",
    "                            elif elem.tag.count(\"}part\") == 1:\n",
    "                                for subelem in elem:\n",
    "                                    if subelem.tag == 'detail':\n",
    "                                        detail_type=subelem.attrib.get('type', None)\n",
    "                                        if detail_type == 'volume':\n",
    "                                            for subsubelem in subelem:\n",
    "                                                if subsubelem.tag == 'number':\n",
    "                                                    titleInfo[f\"volume\"]=subsubelem.text\n",
    "                                        if detail_type == 'issue':\n",
    "                                            for subsubelem in subelem:\n",
    "                                                if subsubelem.tag == 'number':\n",
    "                                                    titleInfo[f\"issue\"]=subsubelem.text\n",
    "                            elif elem.tag.count(\"}genre\") == 1:\n",
    "                                if elem.text is not None:\n",
    "                                    titleInfo['genre']=elem.text\n",
    "                                else:\n",
    "                                    elemattr=elem.attrib.get('type', None)\n",
    "                                    if elemattr:\n",
    "                                        print(f\"genre case in relatedItem for {elem=} with type {elemattr}\")\n",
    "                                    else:\n",
    "                                        print(f\"genre case in relatedItem for {elem=}\")\n",
    "                            else:\n",
    "                                print(f\"Unhanded case in relatedItem for {elem=}\")\n",
    "                    diva_entry[relatedItemType]=titleInfo\n",
    "                elif mod_element.tag.count(\"}physicalDescription\") == 1:\n",
    "                    #<physicalDescription>\n",
    "                    #  <form authority=\"marcform\">electronic</form>\n",
    "                    #  <extent>xii,74</extent></physicalDescription>\n",
    "                    for elem in mod_element:\n",
    "                        if elem.tag.count(\"}extent\") == 1:\n",
    "                            diva_entry['Pages']=elem.text\n",
    "                elif mod_element.tag.count(\"}note\") == 1:\n",
    "                    #<note type=\"level\" lang=\"swe\">Självständigt arbete på avancerad nivå (masterexamen)</note>\n",
    "                    #<note type=\"universityCredits\" lang=\"swe\">20 poäng / 30 hp</note>\n",
    "                    #<note type=\"cooperation\">Saab AB</note>\n",
    "                    notetype=mod_element.attrib.get('type', None)\n",
    "                    if not notetype:\n",
    "                        continue\n",
    "                    elif notetype in ['level', 'universityCredits', 'cooperation', 'venue',  'funder', 'papers',\n",
    "                                      'sustainableDevelopment', 'publicationStatus', 'creatorCount',\n",
    "                                      'version identification'\n",
    "]:\n",
    "                        diva_entry[notetype]=mod_element.text\n",
    "\n",
    "                    elif notetype in ['degree', 'thesis', 'patent', 'project']:\n",
    "                        diva_entry[f\"{notetype}_note\"]=mod_element.text\n",
    "                    elif notetype == 'publicationChannel':\n",
    "                        diva_entry[\"degree_publicationChannel\"]=mod_element.text\n",
    "                    else:\n",
    "                        print(f\"unhandled case of note for {notetype}\")\n",
    "                else:\n",
    "                    print(\"Unhandled case in mod mod_element={}\".format(mod_element))\n",
    "        try:\n",
    "            df2 = pd.json_normalize(diva_entry)\n",
    "        except BaseException as err:\n",
    "            print(f\"Unexpected {err=}, {type(err)=} with {diva_entry=}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df=pd.concat([df, df2], axis=0, join='outer', ignore_index = True, verify_integrity=False)\n",
    "        except BaseException as err:\n",
    "            print(f\"Unexpected {err=}, {type(err)=} with {df2=}\")\n",
    "            continue\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from the fordiva.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kthid='u1XXXXXX' Student, Fake A.\n"
     ]
    }
   ],
   "source": [
    "fordiva_filename=directory_prefix+'fordiva.json'\n",
    "user_info=dict()\n",
    "try:\n",
    "    with open(fordiva_filename, 'r', encoding='utf-8') as json_FH:\n",
    "        json_string=json_FH.read()\n",
    "        user_info=json.loads(json_string)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {fordiva_filename}\")\n",
    "kthid=user_info['Author1']['Local User Id']\n",
    "last_name=user_info['Author1']['Last name']\n",
    "first_name=user_info['Author1']['First name']\n",
    "print(f\"{kthid=} {last_name}, {first_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing purposes we will use my KTHID and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kthid='u1d13i2c'\n",
    "last_name='Maguire'\n",
    "first_name='Gerald'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch a user's data from DiVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url=https://kth.diva-portal.org/smash/export.jsf?format=mods&addFilename=true&aq=[[{\"personId\":\"u1d13i2c\"}]]&aqe=[]&aq2=[[]]&onlyFullText=false&noOfRows=5000&sortOrder=title_sort_asc&sortOrder2=title_sort_asc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(336, 61)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mods_kthid(kthid, filename):\n",
    "        url='https://kth.diva-portal.org/smash/export.jsf?format=mods&addFilename=true&aq=[[{\"personId\":\"'+f\"{kthid}\"+'\"}]]&aqe=[]&aq2=[[]]&onlyFullText=false&noOfRows=5000&sortOrder=title_sort_asc&sortOrder2=title_sort_asc'\n",
    "        print(\"url={}\".format(url))\n",
    "        req = urllib.request.Request(url)\n",
    "        try:\n",
    "            with urllib.request.urlopen(req) as response:\n",
    "                data_str=response.read()\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(e.code)\n",
    "            print(e.read())\n",
    "            raise\n",
    "\n",
    "        with open(filename, \"wb\") as mods_data_file:\n",
    "            mods_data_file.write(data_str)\n",
    "        mods_records = pymods.MODSReader(filename)\n",
    "        return mods_records\n",
    "\n",
    "\n",
    "filename=f\"/tmp/{kthid}-diva-mods\"\n",
    "mods_records=get_mods_kthid(kthid, filename)\n",
    "user_df=mods_records_to_dataframe(mods_records)\n",
    "user_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get information from the references.bib file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ENTRYTYPE': 'inproceedings',\n",
      "  'ID': 'farshin_make_2019',\n",
      "  'address': 'Dresden, Germany',\n",
      "  'author': 'Farshin, Alireza and Roozbeh, Amir and Maguire, Gerald Q. and '\n",
      "            'Kostić, Dejan',\n",
      "  'booktitle': 'Proceedings of the {Fourteenth} {EuroSys} {Conference} 2019 '\n",
      "               \"{CD}-{ROM} on {ZZZ} - {EuroSys} '19\",\n",
      "  'doi': '10.1145/3302424.3303977',\n",
      "  'isbn': '978-1-4503-6281-8',\n",
      "  'language': 'english',\n",
      "  'pages': '1--17',\n",
      "  'publisher': 'ACM Press',\n",
      "  'title': 'Make the {Most} out of {Last} {Level} {Cache} in {Intel} '\n",
      "           '{Processors}',\n",
      "  'url': 'http://dl.acm.org/citation.cfm?doid=3302424.3303977',\n",
      "  'year': '2019'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'ioannidis_coherent_1991',\n",
      "  'abstract': 'This memo describes the Coherent File Distribution Protocol '\n",
      "              '(CFDP). This is an Experimental Protocol for the Internet '\n",
      "              'community. It does not specify an Internet standard.',\n",
      "  'author': 'Ioannidis, J. and Maguire, G.',\n",
      "  'doi': '10.17487/RFC1235',\n",
      "  'issn': '2070-1721',\n",
      "  'journal': 'Internet Request for Comments',\n",
      "  'month': 'June',\n",
      "  'title': 'Coherent {File} {Distribution} {Protocol}',\n",
      "  'url': 'http://www.rfc-editor.org/rfc/rfc1235.txt',\n",
      "  'volume': 'RFC 1235 (Experimental)',\n",
      "  'year': '1991'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'kim_small-mass_2016',\n",
      "  'abstract': 'Neutrino oscillation experiments presently suggest that '\n",
      "              'neutrinos have a small but finite mass. If neutrinos have mass, '\n",
      "              'there should be a Lorentz frame in which they can be brought to '\n",
      "              'rest. This paper discusses how Wigner’s little groups can be '\n",
      "              'used to distinguish between massive and massless particles. We '\n",
      "              'derive a representation of the SL(2,c) group which separates '\n",
      "              'out the two sets of spinors: one set is gauge dependent and the '\n",
      "              'other set is gauge invariant and represents polarized '\n",
      "              'neutrinos. We show that a similar calculation can be done for '\n",
      "              'the Dirac equation. In the large-momentum/zero-mass limit, the '\n",
      "              'Dirac spinors can be separated into large and small components. '\n",
      "              'The large components are gauge invariant, while the small '\n",
      "              'components are not. These small components represent spin-1/2 '\n",
      "              'non-zero-mass particles. If we renormalize the large '\n",
      "              'components, these gauge invariant spinors represent the '\n",
      "              'polarization of neutrinos. Massive neutrinos cannot be '\n",
      "              'invariant under gauge transformations.',\n",
      "  'author': 'Kim, Y. S. and Maguire, G. Q. and Noz, M. E.',\n",
      "  'doi': '10.1155/2016/1847620',\n",
      "  'issn': '1687-7357, 1687-7365',\n",
      "  'journal': 'Advances in High Energy Physics',\n",
      "  'language': 'english',\n",
      "  'pages': '1--7',\n",
      "  'title': 'Do {Small}-{Mass} {Neutrinos} {Participate} in {Gauge} '\n",
      "           '{Transformations}?',\n",
      "  'url': 'http://www.hindawi.com/journals/ahep/2016/1847620/',\n",
      "  'volume': '2016',\n",
      "  'year': '2016'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'maguire_jr_new_2014',\n",
      "  'abstract': 'As the most advantageous total hip arthroplasty (THA) operation '\n",
      "              'is the first, timely replacement of only the liner is socially '\n",
      "              'and economically important because the utilization of THA is '\n",
      "              'increasing as younger and more active patients are receiving '\n",
      "              'implants and they are living longer. Automatic algorithms were '\n",
      "              'developed to infer liner wear by estimating the separation '\n",
      "              'between the acetabular cup and femoral component head given a '\n",
      "              'computed tomography (CT) volume. Two series of CT volumes of a '\n",
      "              'hip phantom were acquired with the femoral component head '\n",
      "              'placed at 14 different positions relative to the acetabular '\n",
      "              'cup. The mean and standard deviation (SD) of the diameter of '\n",
      "              'the acetabular cup and femoral component head, in addition to '\n",
      "              'the range of error in the expected wear values and the '\n",
      "              'repeatability of all the measurements, were calculated. The '\n",
      "              'algorithms resulted in a mean (±SD) for the diameter of the '\n",
      "              'acetabular cup of 54.21 (±0.011) mm and for the femoral '\n",
      "              'component head of 22.09 (±0.02) mm. The wear error was '\n",
      "              '±0.1\\u2009mm and the repeatability was 0.077\\u2009mm. This '\n",
      "              'approach is applicable clinically as it utilizes readily '\n",
      "              'available computed tomography imaging systems and requires only '\n",
      "              'five minutes of human interaction.',\n",
      "  'author': 'Maguire Jr., Gerald Q. and Noz, Marilyn E. and Olivecrona, Henrik '\n",
      "            'and Zeleznik, Michael P. and Weidenhielm, Lars',\n",
      "  'doi': '10.1155/2014/528407',\n",
      "  'issn': '2356-6140, 1537-744X',\n",
      "  'journal': 'The Scientific World Journal',\n",
      "  'language': 'english',\n",
      "  'pages': '1--9',\n",
      "  'shorttitle': 'A {New} {Automated} {Way} to {Measure} {Polyethylene} {Wear} '\n",
      "                'in {THA} {Using} a {High} {Resolution} {CT} {Scanner}',\n",
      "  'title': 'A {New} {Automated} {Way} to {Measure} {Polyethylene} {Wear} in '\n",
      "           '{THA} {Using} a {High} {Resolution} {CT} {Scanner}: {Method} and '\n",
      "           '{Analysis}',\n",
      "  'url': 'http://www.hindawi.com/journals/tswj/2014/528407/',\n",
      "  'volume': '2014',\n",
      "  'year': '2014'},\n",
      " {'ENTRYTYPE': 'inproceedings',\n",
      "  'ID': 'bogdanov_nearest_2015',\n",
      "  'address': 'Kohala Coast, Hawaii',\n",
      "  'author': 'Bogdanov, Kirill and Peón-Quirós, Miguel and Maguire, Gerald Q. '\n",
      "            'and Kostć, Dejan',\n",
      "  'booktitle': 'Proceedings of the {Sixth} {ACM} {Symposium} on {Cloud} '\n",
      "               \"{Computing} - {SoCC} '15\",\n",
      "  'doi': '10.1145/2806777.2806939',\n",
      "  'isbn': '978-1-4503-3651-2',\n",
      "  'language': 'english',\n",
      "  'pages': '16--29',\n",
      "  'publisher': 'ACM Press',\n",
      "  'title': 'The nearest replica can be farther than you think',\n",
      "  'url': 'http://dl.acm.org/citation.cfm?doid=2806777.2806939',\n",
      "  'year': '2015'},\n",
      " {'ENTRYTYPE': 'inproceedings',\n",
      "  'ID': 'roozbeh_resource_2013',\n",
      "  'address': 'Dresden, Germany',\n",
      "  'author': 'Roozbeh, Amir and Sefidcon, Azimeh and Maguire, Gerald Q.',\n",
      "  'booktitle': '2013 {IEEE}/{ACM} 6th {International} {Conference} on '\n",
      "               '{Utility} and {Cloud} {Computing}',\n",
      "  'doi': '10.1109/UCC.2013.36',\n",
      "  'isbn': '978-0-7695-5152-4',\n",
      "  'month': 'December',\n",
      "  'pages': '139--146',\n",
      "  'publisher': 'IEEE',\n",
      "  'shorttitle': 'Resource {Monitoring} in a {Network} {Embedded} {Cloud}',\n",
      "  'title': 'Resource {Monitoring} in a {Network} {Embedded} {Cloud}: {An} '\n",
      "           '{Extension} to {OSPF}-{TE}',\n",
      "  'url': 'http://ieeexplore.ieee.org/document/6809350/',\n",
      "  'year': '2013'},\n",
      " {'ENTRYTYPE': 'mastersthesis',\n",
      "  'ID': 'Mao_Ni_2004',\n",
      "  'author': 'Mao Ni',\n",
      "  'month': '5',\n",
      "  'note': '{A Master’s Paper for the M.S. in I.S. degree, Advisor: Bradley M. '\n",
      "          'Hemminger}',\n",
      "  'pages': '38',\n",
      "  'school': '{University of North Carolina at Chapel Hill, School of Library '\n",
      "            'and Information Science, Chapel Hill, North Carolina, USA}',\n",
      "  'title': '{Automatic extraction of author self contributed metadata for '\n",
      "           'Electronic Theses and Dissertations}',\n",
      "  'url': 'https://ils.unc.edu/MSpapers/2997.pdf',\n",
      "  'year': '20004'},\n",
      " {'ENTRYTYPE': 'misc',\n",
      "  'ID': 'hickey_pavani_suleman_2010',\n",
      "  'author': 'Thom Hickey and Ana Pavani and Hussein Suleman',\n",
      "  'journal': 'NDLTD',\n",
      "  'month': 'August',\n",
      "  'note': 'version 1.1 - the author list is the editors',\n",
      "  'publisher': 'Networked Digital Library of Theses and Dissertations',\n",
      "  'title': '{ETD-MS v1.1}: An interoperability metadata standard for '\n",
      "           'electronic theses and dissertations',\n",
      "  'url': 'https://ndltd.org/wp-content/uploads/2021/04/etd-ms-v1.1.html',\n",
      "  'year': '2010'},\n",
      " {'ENTRYTYPE': 'misc',\n",
      "  'ID': 'digital_commons_metadata_2016',\n",
      "  'month': 'December',\n",
      "  'title': 'Dublin Core Elements in Digital Commons',\n",
      "  'url': 'https://bepress.com/wp-content/uploads/2016/12/Dublin-Core-Elements-in-Digital-Commons.pdf',\n",
      "  'year': '2016'},\n",
      " {'ENTRYTYPE': 'misc',\n",
      "  'ID': 'bepressOAIPMH',\n",
      "  'author': 'bepress',\n",
      "  'title': 'Digital Commons and OAI-PMH: Outbound Harvesting of Repository '\n",
      "           'Records',\n",
      "  'url': 'https://bepress.com/reference_guide_dc/digital-commons-oai-harvesting/'},\n",
      " {'ENTRYTYPE': 'misc',\n",
      "  'ID': 'Ann_Apps_2005',\n",
      "  'author': 'Ann Apps',\n",
      "  'month': 'June',\n",
      "  'note': 'Date Issued:  2005-06-13',\n",
      "  'title': 'Guidelines for Encoding Bibliographic Citation Information in '\n",
      "           'Dublin Core™ Metadata',\n",
      "  'url': 'https://www.dublincore.org/specifications/dublin-core/dc-citation-guidelines/',\n",
      "  'year': '2005'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'ivanovic_data_2012',\n",
      "  'abstract': 'Purpose\\n'\n",
      "              'The aim of this research is to define a data model of theses '\n",
      "              'and dissertations that enables data exchange with '\n",
      "              'CERIF‐compatible CRIS systems and data exchange according to '\n",
      "              'OAI‐PMH protocol in different metadata formats (Dublin Core, '\n",
      "              'EDT‐MS, etc.).\\n'\n",
      "              '\\n'\n",
      "              '\\n'\n",
      "              'Design/methodology/approach\\n'\n",
      "              'Various systems that contain metadata about theses and '\n",
      "              'dissertations are analyzed. There are different standards and '\n",
      "              'protocols that enable the interoperability of those systems: '\n",
      "              'CERIF standard, AOI‐PMH protocol, etc. A physical data model '\n",
      "              'that enables interoperability with almost all of those systems '\n",
      "              'is created using the PowerDesigner CASE tool.\\n'\n",
      "              '\\n'\n",
      "              '\\n'\n",
      "              'Findings\\n'\n",
      "              'A set of metadata about theses and dissertations that contain '\n",
      "              'all the metadata required by CERIF data model, Dublin Core '\n",
      "              'format, EDT‐MS format and all the metadata prescribed by the '\n",
      "              'University of Novi Sad is defined. Defined metadata can be '\n",
      "              'stored in the CERIF‐compatible data model based on the MARC21 '\n",
      "              'format.\\n'\n",
      "              '\\n'\n",
      "              '\\n'\n",
      "              'Practical implications\\n'\n",
      "              'CRIS‐UNS is a CRIS which has been developed at the University '\n",
      "              'of Novi Sad since 2008. The system is based on the proposed '\n",
      "              \"data model, which enables the system's interoperability with \"\n",
      "              'other CERIF‐compatible CRIS systems. Also, the system based on '\n",
      "              'the proposed model can become a member of NDLTD.\\n'\n",
      "              '\\n'\n",
      "              '\\n'\n",
      "              'Social implications\\n'\n",
      "              'A system based on the proposed model increases the availability '\n",
      "              'of theses and dissertations, and thus encourages the '\n",
      "              'development of the knowledge‐based society.\\n'\n",
      "              '\\n'\n",
      "              '\\n'\n",
      "              'Originality/value\\n'\n",
      "              'A data model of theses and dissertations that enables '\n",
      "              'interoperability with CERIF‐compatible CRIS systems is '\n",
      "              'proposed. A software system based on the proposed model could '\n",
      "              'become a member of NDLTD and exchange metadata with '\n",
      "              'institutional repositories. The proposed model increases the '\n",
      "              'availability of theses and dissertations.',\n",
      "  'author': 'Ivanović, Lidija and Ivanović, Dragan and Surla, Dušan',\n",
      "  'doi': '10.1108/14684521211254068',\n",
      "  'issn': '1468-4527',\n",
      "  'journal': 'Online Information Review',\n",
      "  'language': 'en',\n",
      "  'month': 'August',\n",
      "  'number': '4',\n",
      "  'pages': '548--567',\n",
      "  'title': 'A data model of theses and dissertations compatible with {CERIF}, '\n",
      "           '{Dublin} {Core} and {EDT}‐{MS}',\n",
      "  'url': 'https://www.emerald.com/insight/content/doi/10.1108/14684521211254068/full/html',\n",
      "  'volume': '36',\n",
      "  'year': '2012'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'menendez-aller_humor_2020',\n",
      "  'author': 'Álvaro Menéndez-Aller  and Álvaro Postigo and Pelayo '\n",
      "            'Montes-Álvarez  and Francisco José González-Primo and Eduardo '\n",
      "            'García-Cueto',\n",
      "  'doi': '10.1016/j.ijchp.2019.12.002',\n",
      "  'issn': '16972600',\n",
      "  'journal': 'International Journal of Clinical and Health Psychology',\n",
      "  'language': 'en',\n",
      "  'month': 'January',\n",
      "  'number': '1',\n",
      "  'pages': '38--45',\n",
      "  'title': 'Humor as a protective factor against anxiety and depression',\n",
      "  'url': 'https://linkinghub.elsevier.com/retrieve/pii/S1697260019302509',\n",
      "  'volume': '20',\n",
      "  'year': '2020'},\n",
      " {'ENTRYTYPE': 'patent',\n",
      "  'ID': 'US7107453B2',\n",
      "  'abstract': 'Systems and methods for generating and decoding authenticatable '\n",
      "              'graphical bar codes are described. In one aspect, a '\n",
      "              'corroborative signed message is generated from information to '\n",
      "              'be encoded, and a base image is modulated with a graphical '\n",
      "              'encoding of the signed message to produce a marked image. In '\n",
      "              'another aspect, a signed message is extracted from a marked '\n",
      "              'image based upon a comparison of the marked image and a base '\n",
      "              'image. The extracted signed message is decoded to produce a '\n",
      "              'decoded message. Information encoded in the marked image is '\n",
      "              'extracted from the decoded message and authenticated.',\n",
      "  'author': 'Yen, Jonathan and Maguire Jr., Gerald Q. and Saw, Chit Wei and '\n",
      "            'Yihong, Xu',\n",
      "  'holder': 'Hewlett-Packard Development Company, L.P.',\n",
      "  'month': 'December',\n",
      "  'nationality': 'United States',\n",
      "  'note': '{Granted US 7107453 B2 (2006-09-12), EP 1340188-B1 (2007-03-07); JP '\n",
      "          '4495908-B2 (2010-07-07)}',\n",
      "  'number': '7107453B2',\n",
      "  'title': 'Authenticatable graphical bar codes',\n",
      "  'url': 'https://patentimages.storage.googleapis.com/cc/f0/90/578b04737a117e/US7107453.pdf',\n",
      "  'year': '2000'},\n",
      " {'ENTRYTYPE': 'patent',\n",
      "  'ID': 'US12111768B2',\n",
      "  'abstract': 'A method and device for controlling memory handling in a '\n",
      "              'processing system comprising a cache shared between a plurality '\n",
      "              'of processing units, wherein the cache comprises a plurality of '\n",
      "              'cache portions. The method comprises obtaining first '\n",
      "              'information pertaining to an allocation of a first memory '\n",
      "              'portion of a memory to a first application, an allocation of a '\n",
      "              'first processing unit of the plurality of processing units to '\n",
      "              'the first application, and an association between a first cache '\n",
      "              'portion of the plurality of cache portions and the first '\n",
      "              'processing unit. The method further comprises reconfiguring a '\n",
      "              'mapping configuration based on the obtained first information, '\n",
      "              'and controlling a providing of first data associated with the '\n",
      "              'first application to the first cache portion from the first '\n",
      "              'memory portion using the reconfigured mapping configuration. ',\n",
      "  'author': 'Roozbeh, Amir and Farshin, Alireza and Kostić, Dejan and Maguire '\n",
      "            'Jr., Gerald Q.',\n",
      "  'holder': 'Telefonaktiebolaget LM Ericsson AB',\n",
      "  'keywords': 'memory handling, shared cache',\n",
      "  'month': 'February',\n",
      "  'nationality': 'United States',\n",
      "  'note': '{Granted US12111768 B2 (2024-10-08)}',\n",
      "  'number': '12111768',\n",
      "  'title': 'Methods and devices for controlling memory handling',\n",
      "  'year': '2020'},\n",
      " {'ENTRYTYPE': 'inproceedings',\n",
      "  'ID': 'Ghasemirahni_2022',\n",
      "  'address': 'Renton, WA',\n",
      "  'author': 'Hamid Ghasemirahni and Tom Barbette and Georgios P. Katsikas and '\n",
      "            'Alireza Farshin and Amir Roozbeh and Massimo Girondi and Marco '\n",
      "            'Chiesa and Gerald Q. Maguire Jr. and Dejan Kostić',\n",
      "  'booktitle': '19th USENIX Symposium on Networked Systems Design and '\n",
      "               'Implementation (NSDI 22)',\n",
      "  'isbn': '978-1-939133-27-4',\n",
      "  'month': 'April',\n",
      "  'pages': '807--827',\n",
      "  'publisher': 'USENIX Association',\n",
      "  'title': 'Packet Order Matters! Improving Application Performance by '\n",
      "           'Deliberately Delaying Packets',\n",
      "  'url': 'https://www.usenix.org/conference/nsdi22/presentation/ghasemirahni',\n",
      "  'year': '2022'},\n",
      " {'ENTRYTYPE': 'misc',\n",
      "  'ID': 'reframer',\n",
      "  'author': 'Hamid Ghasemirahni',\n",
      "  'note': 'Notw: Reframer is a FastClick based Network Function (NF) that '\n",
      "          'leverages the idea of briefly buffering, delaying, and reordering '\n",
      "          'the (possibly paced) incoming packets to increase spatial locality '\n",
      "          'for network traffic.',\n",
      "  'title': 'Reframer',\n",
      "  'url': 'https://github.com/hamidgh09/Reframer',\n",
      "  'year': '2022'},\n",
      " {'ENTRYTYPE': 'inproceedings',\n",
      "  'ID': '10.1145/3445814.3446724',\n",
      "  'abstract': 'We present PacketMill, a system for optimizing software packet '\n",
      "              'processing, which (i) introduces a new model to efficiently '\n",
      "              'manage packet metadata and (ii) employs code-optimization '\n",
      "              'techniques to better utilize commodity hardware. PacketMill '\n",
      "              'grinds the whole packet processing stack, from the high-level '\n",
      "              'network function configuration file to the low-level userspace '\n",
      "              'network (specifically DPDK) drivers, to mitigate inefficiencies '\n",
      "              'and produce a customized binary for a given network function. '\n",
      "              'Our evaluation results show that PacketMill increases '\n",
      "              'throughput (up to 36.4 Gbps -- 70\\\\%) \\\\& reduces latency (up '\n",
      "              'to 101 us -- 28\\\\%) and enables nontrivial packet processing '\n",
      "              '(e.g., router) at ~100 Gbps, when new packets arrive '\n",
      "              '>10\\\\texttimes{} faster than main memory access times, while '\n",
      "              'using only one processing core.',\n",
      "  'address': 'New York, NY, USA',\n",
      "  'author': 'Farshin, Alireza and Barbette, Tom and Roozbeh, Amir and Maguire '\n",
      "            'Jr., Gerald Q. and Kostić, Dejan',\n",
      "  'booktitle': 'Proceedings of the 26th ACM International Conference on '\n",
      "               'Architectural Support for Programming Languages and Operating '\n",
      "               'Systems',\n",
      "  'doi': '10.1145/3445814.3446724',\n",
      "  'isbn': '9781450383172',\n",
      "  'keywords': '100-Gbps Networking, Commodity Hardware, Compiler '\n",
      "              'Optimizations, DPDK, FastClick, Full-Stack Optimization, LLVM, '\n",
      "              'Metadata Management, Middleboxes, Packet Processing, '\n",
      "              'PacketMill, X-Change',\n",
      "  'numpages': '17',\n",
      "  'pages': '1–17',\n",
      "  'publisher': 'Association for Computing Machinery',\n",
      "  'series': \"ASPLOS '21\",\n",
      "  'title': 'PacketMill: toward per-Core {100-Gbps} networking',\n",
      "  'url': 'https://doi.org/10.1145/3445814.3446724',\n",
      "  'year': '2021'},\n",
      " {'ENTRYTYPE': 'software',\n",
      "  'ID': '10.5281/zenodo.4435970',\n",
      "  'abstract': '\\n'\n",
      "              '<p>This is the artifact for the “PacketMill: Toward per-core '\n",
      "              '100-Gbps Networking” paper published at ASPLOS’21.</p>\\n'\n",
      "              '<p>PacketMill is a system that optimizes the performance of '\n",
      "              'network functions via holistic inter-stack optimizations. More '\n",
      "              'specifically, PacketMill provides a new metadata management '\n",
      "              'model, called X-Change, enabling the packet processing '\n",
      "              'frameworks to provide their custom buffer to DPDK and fully '\n",
      "              'bypass rte_mbuf. Additionally, PacketMill performs a set of '\n",
      "              'source-code \\\\& intermediate representation (IR) code '\n",
      "              'optimizations.</p>\\n'\n",
      "              '<p>Our paper’s artifact contains the source code, the '\n",
      "              'experimental workflow, and additional information to (i) set '\n",
      "              'upPacketMill \\\\& its testbed, (ii) perform some of the '\n",
      "              'experiments presented in the paper, and (iii) validates the '\n",
      "              'reusability \\\\& effectiveness of PacketMill.</p>\\n'\n",
      "              '<p>For more information, please refer to '\n",
      "              'https://github.com/aliireza/packetmill</p>\\n',\n",
      "  'author': 'Farshin, Alireza and Barbette, Tom and Roozbeh, Amir and Maguire '\n",
      "            'Jr., Gerald Q. and Kostić, Dejan',\n",
      "  'keywords': 'DPDK., FastClick, LLVM, Middleboxes, Packet Processing, '\n",
      "              'PacketMill, X-Change',\n",
      "  'publisher': 'Association for Computing Machinery',\n",
      "  'title': '{PacketMill: Toward Per-Core 100-Gbps Networking - Artifact for '\n",
      "           \"ASPLOS'21}\",\n",
      "  'url': 'https://doi.org/10.5281/zenodo.4435970',\n",
      "  'year': '2021'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'Noz_1975',\n",
      "  'author': 'Noz, Marilyn E. and Maguire Jr., Gerald Q.',\n",
      "  'doi': '10.1016/0020-708x(75)90145-3',\n",
      "  'institution': 'New York University',\n",
      "  'journal': '{The International Journal of Applied Radiation and Isotopes}',\n",
      "  'kthbnote': 'QC 20111115Letter',\n",
      "  'number': '12',\n",
      "  'pages': '785--786',\n",
      "  'title': '{Calculation of $\\\\bar{E}_{\\\\beta}$, $\\\\Gamma$ and $\\\\Delta_{i}$ '\n",
      "           'for $^{99m}$Tc}',\n",
      "  'volume': '26',\n",
      "  'year': '1975'},\n",
      " {'ENTRYTYPE': 'inproceedings',\n",
      "  'ID': 'Panko2015',\n",
      "  'author': 'Raymond R. Panko',\n",
      "  'booktitle': 'Proceedings of the {EuSpRIG} 2015 Conference “Spreadsheet Risk '\n",
      "               'Management”',\n",
      "  'doi': 'https://doi.org/10.48550/arXiv.1602.02601',\n",
      "  'isbn': '978-1-905404-52-0',\n",
      "  'organization': 'European Spreadsheet Risks Interest Group '\n",
      "                  '(www.eusprig.org) ',\n",
      "  'pages': '79-93',\n",
      "  'title': 'What We Don’t Know About Spreadsheet Errors Today:\\n'\n",
      "           'The Facts, Why We Don’t Believe Them, and What We Need to Do',\n",
      "  'url': 'https://arxiv.org/pdf/1602.02601',\n",
      "  'year': '2015'},\n",
      " {'ENTRYTYPE': 'misc',\n",
      "  'ID': 'verardo2023fmmheadenhancingautoencoderbasedecg',\n",
      "  'archiveprefix': 'arXiv',\n",
      "  'author': 'Giacomo Verardo and Magnus Boman and Samuel Bruchfeld and Marco '\n",
      "            'Chiesa and Sabine Koch and Gerald Q. {Maguire Jr.} and Dejan '\n",
      "            'Kostic',\n",
      "  'eprint': '2310.05848',\n",
      "  'primaryclass': 'cs.LG',\n",
      "  'title': '{FMM-Head}: Enhancing Autoencoder-based {ECG} anomaly detection '\n",
      "           'with prior knowledge',\n",
      "  'url': 'https://arxiv.org/abs/2310.05848',\n",
      "  'year': '2023'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': '10.1145/2209249.2209262',\n",
      "  'abstract': 'By closely connecting research and development Google is able '\n",
      "              'to conduct experiments on an unprecedented scale, often '\n",
      "              'resulting in new capabilities for the company.',\n",
      "  'address': 'New York, NY, USA',\n",
      "  'author': 'Spector, Alfred and Norvig, Peter and Petrov, Slav',\n",
      "  'doi': '10.1145/2209249.2209262',\n",
      "  'issn': '0001-0782',\n",
      "  'issue_date': 'July 2012',\n",
      "  'journal': 'Commun. ACM',\n",
      "  'month': 'July',\n",
      "  'number': '7',\n",
      "  'numpages': '4',\n",
      "  'pages': '34–37',\n",
      "  'publisher': 'Association for Computing Machinery',\n",
      "  'title': \"Google's hybrid approach to research\",\n",
      "  'url': 'https://doi.org/10.1145/2209249.2209262',\n",
      "  'volume': '55',\n",
      "  'year': '2012'},\n",
      " {'ENTRYTYPE': 'book',\n",
      "  'ID': 'SwePubMODS2023',\n",
      "  'author': 'National Library of Sweden',\n",
      "  'edition': 'Version 4.0',\n",
      "  'month': 'August',\n",
      "  'publisher': 'National Library of Sweden',\n",
      "  'title': 'Swepub MODS format specification',\n",
      "  'url': 'https://kb.se/namespace/swepub/mods-format-specification/SWP_MODS_4.pdf',\n",
      "  'year': '2023'},\n",
      " {'ENTRYTYPE': 'book',\n",
      "  'ID': 'SwePubMODS2019',\n",
      "  'author': 'National Library of Sweden',\n",
      "  'edition': 'Version 3.0',\n",
      "  'month': 'September',\n",
      "  'publisher': 'National Library of Sweden',\n",
      "  'title': 'Swepub MODS format specification',\n",
      "  'url': 'https://www.kb.se/namespace/swepub/mods-format-specification/SWP_MODS_3.pdf',\n",
      "  'year': '2019'},\n",
      " {'ENTRYTYPE': 'mastersthesis',\n",
      "  'ID': 'BesharatPour_and_Li_2019',\n",
      "  'author': 'Besharat  Pour, Shiva and Li, Qi',\n",
      "  'keywords': 'RESTful APIs, Canvas, DiVA, Calendar announcement, data mining, '\n",
      "              'RESTful APIs, Canvas, DiVA, kalendrar, data mining',\n",
      "  'number': '2018:164',\n",
      "  'pages': 'xiii,101',\n",
      "  'school': 'KTH Royal Institute of Technology, School of Electrical '\n",
      "            'Engineering and Computer Science',\n",
      "  'series': 'TRITA-EECS-EX',\n",
      "  'title': '{Connecting Silos: Automation system for thesis processing in '\n",
      "           'Canvas and DiVA}',\n",
      "  'type': \"Bachelor's Thesis\",\n",
      "  'url': 'https://urn.kb.se/resolve?urn=urn%3Anbn%3Ase%3Akth%3Adiva-230996',\n",
      "  'year': '2018'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'MittelbachRowley2020',\n",
      "  'author': 'Frank Mittelbach and Chris Rowley',\n",
      "  'journal': 'TUGboat',\n",
      "  'month': 'October',\n",
      "  'number': '3',\n",
      "  'numpages': '7',\n",
      "  'pages': '292–298',\n",
      "  'publisher': '{TeX Users Group}',\n",
      "  'title': '{LATEX Tagged PDF|A blueprint for a large project}',\n",
      "  'url': 'https://www.latex-project.org/publications/2020-FMi-TUB-tb129mitt-tagpdf.pdf',\n",
      "  'volume': '41',\n",
      "  'year': '2020'},\n",
      " {'ENTRYTYPE': 'techreport',\n",
      "  'ID': 'UAX9',\n",
      "  'address': 'Mountain View, California, U.S.',\n",
      "  'author': 'Davis, Mark',\n",
      "  'institution': 'Unicode, Inc.',\n",
      "  'month': 'September',\n",
      "  'note': 'version       Unicode 16.0.0',\n",
      "  'number': '9',\n",
      "  'title': '{Unicode® Standard Annex \\\\#9: The Bidirectional Algorithm}',\n",
      "  'url': 'http://www.unicode.org/unicode/reports/tr9/',\n",
      "  'year': '2024'},\n",
      " {'ENTRYTYPE': 'techreport',\n",
      "  'ID': 'ISOIEC_29500_1:2016',\n",
      "  'address': 'ISO Central Secretariat, Chemin de Blandonnet 8, CP 401, 1214 '\n",
      "             'Vernier, Geneva, Switzerland',\n",
      "  'annote': 'Edition 4',\n",
      "  'author': '{ISO/IEC JTC 1, Information technology, Subcommittee SC 34, '\n",
      "            'Document description and processing languages}',\n",
      "  'institution': 'International Organization for Standardization and '\n",
      "                 'International Electrotechnical Commission',\n",
      "  'number': 'ISO/IEC 29500-1:2016',\n",
      "  'pages': '5024',\n",
      "  'title': 'Information technology — Document description and processing '\n",
      "           'languages — Office Open XML File Formats; Part 1: Fundamentals and '\n",
      "           'Markup Language Reference',\n",
      "  'type': 'Standard',\n",
      "  'year': '2016'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'brand_beyond_2015',\n",
      "  'abstract': 'Key points\\n'\n",
      "              'As the number of authors on scientific publications increases, '\n",
      "              'ordered lists of author names are proving inadequate for the '\n",
      "              'purposes of attribution and credit.\\n'\n",
      "              'A multi‐stakeholder group has produced a contributor role '\n",
      "              'taxonomy for use in scientific publications.\\n'\n",
      "              'Identifying specific contributions to published research will '\n",
      "              'lead to appropriate credit, fewer author disputes, and fewer '\n",
      "              'disincentives to collaboration and the sharing of data and '\n",
      "              'code.',\n",
      "  'author': 'Brand, Amy and Allen, Liz and Altman, Micah and Hlava, Marjorie '\n",
      "            'and Scott, Jo',\n",
      "  'doi': '10.1087/20150211',\n",
      "  'issn': '0953-1513, 1741-4857',\n",
      "  'journal': 'Learned Publishing',\n",
      "  'language': 'en',\n",
      "  'month': 'April',\n",
      "  'number': '2',\n",
      "  'pages': '151--155',\n",
      "  'shorttitle': 'Beyond authorship',\n",
      "  'title': 'Beyond authorship: attribution, contribution, collaboration, and '\n",
      "           'credit',\n",
      "  'url': 'https://onlinelibrary.wiley.com/doi/10.1087/20150211',\n",
      "  'urldate': '2025-08-25',\n",
      "  'volume': '28',\n",
      "  'year': '2015'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'FakePub2025',\n",
      "  'address': 'Stockholm, Sweden',\n",
      "  'author': 'Gerald Q. Maguire Jr.',\n",
      "  'language': 'english',\n",
      "  'month': 'September',\n",
      "  'title': 'A Fake Publication for testing',\n",
      "  'year': '2025'},\n",
      " {'ENTRYTYPE': 'patent',\n",
      "  'ID': 'patentapplication2002',\n",
      "  'abstract': 'A system includes a network device, and a locator for '\n",
      "              'determining location of the network device. The network device '\n",
      "              'includes a network interface for communication with the '\n",
      "              'locator, and a motion sensor and controller for detecting '\n",
      "              'motion of the network device. The controller sends a location '\n",
      "              'request to the locator when motion of the network device has '\n",
      "              'been detected.',\n",
      "  'author': 'Mark T. Smith and Gerald Q. Maguire Jr.',\n",
      "  'language': 'english',\n",
      "  'month': 'October',\n",
      "  'nationality': 'United States',\n",
      "  'number': '20040080412',\n",
      "  'title': 'Location requests by a network device',\n",
      "  'year': '2002'},\n",
      " {'ENTRYTYPE': 'patent',\n",
      "  'ID': 'patentapplication2019',\n",
      "  'abstract': 'A memory allocator in a computer system comprising a plurality '\n",
      "              'of CPU cores (5101-5104) and a first (530) and a second (5120) '\n",
      "              'memory unit having different data access times and wherein each '\n",
      "              'one of the first and the second memory units is divided into '\n",
      "              'memory portions wherein each memory portion (SLICE 0-3) in the '\n",
      "              'second memory unit is associated with at least one memory '\n",
      "              'portion (A- G) in the first memory unit, and wherein each '\n",
      "              'memory portion in the second memory unit is associated with a '\n",
      "              'CPU core. If at least a predetermined number of memory portions '\n",
      "              'in the first memory unit being part of the available requested '\n",
      "              'memory is associated with the memory portion in the second '\n",
      "              'memory unit that is associated with the CPU core on which the '\n",
      "              'requesting application is running, the requested available '\n",
      "              'memory is allocated to the requesting application.',\n",
      "  'author': 'Amir Roozbeh and Dejan Kostic and Gerald Q Maguire Jr and Alireza '\n",
      "            'Farshin',\n",
      "  'language': 'english',\n",
      "  'month': 'June',\n",
      "  'nationality': 'WIPO',\n",
      "  'number': 'WO2019245445A1',\n",
      "  'title': 'Memory allocation in a hierarchical memory system',\n",
      "  'year': '2019'},\n",
      " {'ENTRYTYPE': 'techreport',\n",
      "  'ID': 'Lee2015',\n",
      "  'abstract': 'Positron emission tomography (PET) studies acquired in list '\n",
      "              'mode offer the opportunity to provide a cine loop showing the '\n",
      "              'dynamics of 18F- PET uptake, giving a visualization of regional '\n",
      "              'bone remodeling. The focus of this report is a group of '\n",
      "              'patients treated with Taylor spatial frames (TSF). The studies '\n",
      "              'were acquired for a period of 45 minutes and saved in list '\n",
      "              'mode. The list was decoded and subsequently segmented into time '\n",
      "              'intervals of one minute each. For each time interval a sinogram '\n",
      "              'was generated from which volumes of one minute each were '\n",
      "              'reconstructed. Slices projected from these volumes could then '\n",
      "              'be displayed as a dynamic loop superimposed on the '\n",
      "              'corresponding computed tomography (CT) slice in order to '\n",
      "              'visualize the 18F- uptake insitu. It was indicated that this '\n",
      "              'technique has the potential of becoming an additional technique '\n",
      "              'to that of using static volumes and SUV values only. As the '\n",
      "              'list mode data was decoded it also offered a method to evaluate '\n",
      "              'the potential decrease in injected activity by eliminating '\n",
      "              'every Nth event from the list before reconstructing the 45 '\n",
      "              'minute volume. This was done and the indication was that the '\n",
      "              'injected activity and hence the effective dose to the patient '\n",
      "              'can be decreased. However, in this work, this was not proven '\n",
      "              'clinically. The open source STIR software was used to '\n",
      "              'reconstruct volumes from sinograms to enable an unlimited '\n",
      "              'access to reconstructing volumes without disturbing the daily '\n",
      "              'routine at the clinic. The data was acquired on a clinical '\n",
      "              'Siemens Medical Solutions Biograph 64 TruePoint TrueV, PET/CT '\n",
      "              'scanner situated at the Nuclear Medicine Department at the '\n",
      "              'Karolinska University Hospital in Solna. This scanner was not '\n",
      "              'supported by the STIR software, hence the data collected by the '\n",
      "              'Siemens PET/CT scanner was translated so that 3D '\n",
      "              'reconstructions could be computed using the STIR tools. The '\n",
      "              'reconstructions made in STIR resulted in volumes of sufficient '\n",
      "              'visual quality, but not as good as those reconstructed by the '\n",
      "              'scanner itself. Further optimization in STIR was left for '\n",
      "              'future work. According to the physicians who treat these '\n",
      "              'patients, dynamic visualization was of sufficient interest to '\n",
      "              'continue to develop and optimize this method. The cine loops '\n",
      "              'that were presented to the physicians were made from JPEG '\n",
      "              'slices produced from the one minute volumes and put together as '\n",
      "              'GIF files. It was also possible to vary the reconstruction time '\n",
      "              '(from uniformly one minute) as well as the presentation rate in '\n",
      "              'the cine loop, but this was left for future work. Ultimately, '\n",
      "              'the cine loop will be implemented in the locally developed '\n",
      "              'software tool. ',\n",
      "  'author': 'Lee, Sabine and Noz, Marilyn E. and Maguire Jr., Gerald Q.',\n",
      "  'institution': 'KTH, Radio Systems Laboratory (RS Lab)',\n",
      "  'keywords': 'PET, Image Reconstruction, Dynamic Visualization, List Mode, '\n",
      "              'STIR, PET, Bildrekonstruktion, Dynamisk visualisering, List '\n",
      "              'mode, STIR',\n",
      "  'number': '2015:24',\n",
      "  'pages': 'xvii,131',\n",
      "  'series': 'TRITA-ICT',\n",
      "  'title': 'Dynamic PET visualization of bone remodeling using NaF-18',\n",
      "  'year': '2015'},\n",
      " {'ENTRYTYPE': 'article',\n",
      "  'ID': 'Khatib2003',\n",
      "  'abstract': 'Since the approval of the IEEE 802.11b by the IEEE in 1999, the '\n",
      "              'demand for WLAN equipment and networks has been growing '\n",
      "              'quickly. We present a queuing model of wireless LAN (WLAN) '\n",
      "              'access points (APs) for IEEE 802.11b. We use experimentation to '\n",
      "              'obtain the characteristic parameters of our analytic model. The '\n",
      "              'model can be used to compare the performance of different WLAN '\n",
      "              'APs as well as the QoS of different applications in the '\n",
      "              'presence of an AP. We focus on the delay introduced by an AP. '\n",
      "              'The major observations are that the delay to serve a packet '\n",
      "              'going from the WLAN medium to the wired medium (on the uplink) '\n",
      "              'is less than the delay to serve a packet, with identical '\n",
      "              'payload, but travelling from the wired medium to the WLAN '\n",
      "              'medium (on the downlink). A key result is an analytic solution '\n",
      "              'showing that the average service time of a packet is a strictly '\n",
      "              'increasing function of payload.',\n",
      "  'author': 'Khatib, Iyad Al and Maguire Jr., Gerald Q. and Ayani, Rassul and '\n",
      "            'Forsgren, Daniel',\n",
      "  'doi': '10.1145/881978.881985',\n",
      "  'journal': 'ACM SIGMOBILE Mobile Computing and Communications Review',\n",
      "  'note': 'Poster',\n",
      "  'number': '1',\n",
      "  'pages': '28--30',\n",
      "  'title': 'MobiCom poster : wireless LAN access points as queuing systems: '\n",
      "           'performance analysis and service time',\n",
      "  'volume': '7',\n",
      "  'year': '2003'}]\n"
     ]
    }
   ],
   "source": [
    "# get the existing bib data\n",
    "bibtext_filename=directory_prefix+'references.bib'\n",
    "bibtex_string=''\n",
    "try:\n",
    "    with open(bibtext_filename, 'r', encoding='utf-8') as bibtex_FH:\n",
    "        bibtex_string=bibtex_FH.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {bibtext_filename}\")\n",
    "\n",
    "if Verbose_Flag:\n",
    "    print(\"read bibtex file: {}\".format(d))\n",
    "\n",
    "# Create a parser object\n",
    "parser = bibtexparser.bparser.BibTexParser(ignore_nonstandard_types=False)\n",
    "\n",
    "# Use this custom parser to load the data\n",
    "bib_database = bibtexparser.loads(bibtex_string, parser=parser)\n",
    "\n",
    "# The 'entries' list will now correctly contain all your data\n",
    "pprint(bib_database.entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to do normalization and matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Match Process ---\n",
      "\n",
      "Processing BibTeX key: farshin_make_2019\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1145/3302424.3303977'\n",
      "    - Normalized .bib value: '10.1145/3302424.3303977'\n",
      "    -> SUCCESS: Found DiVA ID diva2:1291291\n",
      "Processing BibTeX key: ioannidis_coherent_1991\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.17487/RFC1235'\n",
      "    - Normalized .bib value: '10.17487/rfc1235'\n",
      "    -> SUCCESS: Found DiVA ID diva2:461420\n",
      "Processing BibTeX key: kim_small-mass_2016\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1155/2016/1847620'\n",
      "    - Normalized .bib value: '10.1155/2016/1847620'\n",
      "    -> SUCCESS: Found DiVA ID diva2:948742\n",
      "Processing BibTeX key: maguire_jr_new_2014\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1155/2014/528407'\n",
      "    - Normalized .bib value: '10.1155/2014/528407'\n",
      "    -> SUCCESS: Found DiVA ID diva2:690828\n",
      "Processing BibTeX key: bogdanov_nearest_2015\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1145/2806777.2806939'\n",
      "    - Normalized .bib value: '10.1145/2806777.2806939'\n",
      "    -> SUCCESS: Found DiVA ID diva2:844010\n",
      "Processing BibTeX key: roozbeh_resource_2013\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1109/UCC.2013.36'\n",
      "    - Normalized .bib value: '10.1109/ucc.2013.36'\n",
      "    -> SUCCESS: Found DiVA ID diva2:675824\n",
      "Processing BibTeX key: Mao_Ni_2004\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: hickey_pavani_suleman_2010\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: digital_commons_metadata_2016\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: bepressOAIPMH\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: Ann_Apps_2005\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: ivanovic_data_2012\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1108/14684521211254068'\n",
      "    - Normalized .bib value: '10.1108/14684521211254068'\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: menendez-aller_humor_2020\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1016/j.ijchp.2019.12.002'\n",
      "    - Normalized .bib value: '10.1016/j.ijchp.2019.12.002'\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: US7107453B2\n",
      "Processing BibTeX key: US12111768B2\n",
      "Processing BibTeX key: Ghasemirahni_2022\n",
      "Processing BibTeX key: reframer\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: 10.1145/3445814.3446724\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1145/3445814.3446724'\n",
      "    - Normalized .bib value: '10.1145/3445814.3446724'\n",
      "    -> SUCCESS: Found DiVA ID diva2:1527198\n",
      "Processing BibTeX key: 10.5281/zenodo.4435970\n",
      "diva2:1527198: ['Software url: https://dl.acm.org/do/10.5281/zenodo.4435970/abs/', 'fulltext:postprint url: https://kth.diva-portal.org/smash/get/diva2:1527198/FULLTEXT01.pdf', 'fulltext:postprint url: https://kth.diva-portal.org/smash/get/diva2:1527198/FULLTEXT02.pdf'] <class 'list'> bib_pseudo_doi='10.5281/zenodo.4435970'\n",
      "--------------------\n",
      "Processing BibTeX key: Noz_1975\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1016/0020-708x(75)90145-3'\n",
      "    - Normalized .bib value: '10.1016/0020-708x(75)90145-3'\n",
      "    -> SUCCESS: Found DiVA ID diva2:456505\n",
      "Processing BibTeX key: Panko2015\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      'https://doi.org/10.48550/arXiv.1602.02601'\n",
      "    - Normalized .bib value: '10.48550/arxiv.1602.02601'\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: verardo2023fmmheadenhancingautoencoderbasedecg\n",
      "Processing BibTeX key: 10.1145/2209249.2209262\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1145/2209249.2209262'\n",
      "    - Normalized .bib value: '10.1145/2209249.2209262'\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: SwePubMODS2023\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: SwePubMODS2019\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: BesharatPour_and_Li_2019\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: MittelbachRowley2020\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: UAX9\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: ISOIEC_29500_1:2016\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: brand_beyond_2015\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1087/20150211'\n",
      "    - Normalized .bib value: '10.1087/20150211'\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: FakePub2025\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: patentapplication2002\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: patentapplication2019\n",
      "  -> FAILED: No match found for this entry.\n",
      "--------------------\n",
      "Processing BibTeX key: Lee2015\n",
      "Processing BibTeX key: Khatib2003\n",
      "  Attempting to match on 'doi'...\n",
      "    - Raw .bib value:      '10.1145/881978.881985'\n",
      "    - Normalized .bib value: '10.1145/881978.881985'\n",
      "    -> SUCCESS: Found DiVA ID diva2:456494\n",
      "\n",
      "--- Final Matching Results ---\n",
      "{'10.1145/2209249.2209262': {'DiVA_ID': 'Not found in database',\n",
      "                             'match_method': 'None'},\n",
      " '10.1145/3445814.3446724': {'DiVA_ID': 'diva2:1527198',\n",
      "                             'match_method': 'Identifier (doi)'},\n",
      " '10.5281/zenodo.4435970': {'DiVA_ID': 'diva2:1527198',\n",
      "                            'match_method': 'Software link (url: '\n",
      "                                            'https://dl.acm.org/do/10.5281/zenodo.4435970/abs/)'},\n",
      " 'Ann_Apps_2005': {'DiVA_ID': 'Not found in database', 'match_method': 'None'},\n",
      " 'BesharatPour_and_Li_2019': {'DiVA_ID': 'Not found in database',\n",
      "                              'match_method': 'None'},\n",
      " 'FakePub2025': {'DiVA_ID': 'Not found in database', 'match_method': 'None'},\n",
      " 'Ghasemirahni_2022': {'DiVA_ID': 'diva2:1609944',\n",
      "                       'match_method': 'Fuzzy Title (Score: 100)'},\n",
      " 'ISOIEC_29500_1:2016': {'DiVA_ID': 'Not found in database',\n",
      "                         'match_method': 'None'},\n",
      " 'Khatib2003': {'DiVA_ID': 'diva2:456494', 'match_method': 'Identifier (doi)'},\n",
      " 'Lee2015': {'DiVA_ID': 'diva2:866274',\n",
      "             'match_method': 'Fuzzy Title (Score: 100)'},\n",
      " 'Mao_Ni_2004': {'DiVA_ID': 'Not found in database', 'match_method': 'None'},\n",
      " 'MittelbachRowley2020': {'DiVA_ID': 'Not found in database',\n",
      "                          'match_method': 'None'},\n",
      " 'Noz_1975': {'DiVA_ID': 'diva2:456505', 'match_method': 'Identifier (doi)'},\n",
      " 'Panko2015': {'DiVA_ID': 'Not found in database', 'match_method': 'None'},\n",
      " 'SwePubMODS2019': {'DiVA_ID': 'Not found in database', 'match_method': 'None'},\n",
      " 'SwePubMODS2023': {'DiVA_ID': 'Not found in database', 'match_method': 'None'},\n",
      " 'UAX9': {'DiVA_ID': 'Not found in database', 'match_method': 'None'},\n",
      " 'US12111768B2': {'DiVA_ID': 'diva2:1926019',\n",
      "                  'match_method': 'Fuzzy Title (Score: 100)'},\n",
      " 'US7107453B2': {'DiVA_ID': 'diva2:456465',\n",
      "                 'match_method': 'Fuzzy Title (Score: 100)'},\n",
      " 'bepressOAIPMH': {'DiVA_ID': 'Not found in database', 'match_method': 'None'},\n",
      " 'bogdanov_nearest_2015': {'DiVA_ID': 'diva2:844010',\n",
      "                           'match_method': 'Identifier (doi)'},\n",
      " 'brand_beyond_2015': {'DiVA_ID': 'Not found in database',\n",
      "                       'match_method': 'None'},\n",
      " 'digital_commons_metadata_2016': {'DiVA_ID': 'Not found in database',\n",
      "                                   'match_method': 'None'},\n",
      " 'farshin_make_2019': {'DiVA_ID': 'diva2:1291291',\n",
      "                       'match_method': 'Identifier (doi)'},\n",
      " 'hickey_pavani_suleman_2010': {'DiVA_ID': 'Not found in database',\n",
      "                                'match_method': 'None'},\n",
      " 'ioannidis_coherent_1991': {'DiVA_ID': 'diva2:461420',\n",
      "                             'match_method': 'Identifier (doi)'},\n",
      " 'ivanovic_data_2012': {'DiVA_ID': 'Not found in database',\n",
      "                        'match_method': 'None'},\n",
      " 'kim_small-mass_2016': {'DiVA_ID': 'diva2:948742',\n",
      "                         'match_method': 'Identifier (doi)'},\n",
      " 'maguire_jr_new_2014': {'DiVA_ID': 'diva2:690828',\n",
      "                         'match_method': 'Identifier (doi)'},\n",
      " 'menendez-aller_humor_2020': {'DiVA_ID': 'Not found in database',\n",
      "                               'match_method': 'None'},\n",
      " 'patentapplication2002': {'DiVA_ID': 'Not found in database',\n",
      "                           'match_method': 'None'},\n",
      " 'patentapplication2019': {'DiVA_ID': 'Not found in database',\n",
      "                           'match_method': 'None'},\n",
      " 'reframer': {'DiVA_ID': 'Not found in database', 'match_method': 'None'},\n",
      " 'roozbeh_resource_2013': {'DiVA_ID': 'diva2:675824',\n",
      "                           'match_method': 'Identifier (doi)'},\n",
      " 'verardo2023fmmheadenhancingautoencoderbasedecg': {'DiVA_ID': 'diva2:1944107',\n",
      "                                                    'match_method': 'Fuzzy '\n",
      "                                                                    'Title '\n",
      "                                                                    '(Score: '\n",
      "                                                                    '99)'}}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. More Robust Normalization Function ---\n",
    "def normalize_identifier(identifier):\n",
    "    \"\"\"Strips common prefixes and handles empty strings.\"\"\"\n",
    "    if not isinstance(identifier, str) or identifier.strip() == \"\":\n",
    "        return None # Return None for empty or non-string data\n",
    "    identifier = re.sub(r'^(https?://)?(doi.org/)?(doi:)?', '', identifier, flags=re.IGNORECASE)\n",
    "    return identifier.lower().strip()\n",
    "\n",
    "def normalize_title(title, subtitle=None, delimiter_pattern=r'[:—-]' ):\n",
    "    if pd.notna(subtitle):\n",
    "        full_title = f\"{title} {subtitle}\"\n",
    "    else:\n",
    "        full_title = str(title)\n",
    "    \n",
    "    parts = re.split(delimiter_pattern, full_title, 1)\n",
    "    cleaned_parts = [re.sub(r'[^\\w\\s]', '', part).lower().strip() for part in parts]\n",
    "    \n",
    "    return ' '.join(cleaned_parts)\n",
    "\n",
    "# --- 2. Pre-process the DataFrame ---\n",
    "identifier_cols = ['doi'] # Add 'url', 'isbn', etc. as needed\n",
    "def preprocess_dataframe(df_to_process):\n",
    "    \"\"\"\n",
    "    Adds normalized columns to the DataFrame for efficient searching.\n",
    "    \"\"\"\n",
    "    # *** THE CRITICAL FIX ***\n",
    "    # Create an explicit copy to work on. This prevents the SettingWithCopyWarning.\n",
    "    df_processed = df_to_process.copy()\n",
    "\n",
    "    # List of identifier columns to check\n",
    "    identifier_cols = ['doi', 'url', 'isbn', 'pmid']\n",
    "    for col in identifier_cols:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[f'norm_{col}'] = df_processed[col].apply(normalize_identifier)\n",
    "\n",
    "    # List of title/subtitle language pairs to check\n",
    "    title_cols = {\n",
    "        'eng': ('title.eng', 'subtitle.eng'),\n",
    "        'swe': ('title.swe', 'subtitle.swe')\n",
    "    }\n",
    "    for lang, (title_col, sub_col) in title_cols.items():\n",
    "        if title_col in df_processed.columns:\n",
    "            if sub_col in df_processed.columns:\n",
    "                df_processed[f'norm_title_{lang}'] = df_processed.apply(lambda row: normalize_title(row[title_col], row[sub_col]), axis=1)\n",
    "            else:\n",
    "                df_processed[f'norm_title_{lang}'] = df_processed[title_col].apply(normalize_title)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "    \n",
    "# --- 3. Main Matching Logic with Debugging ---\n",
    "def find_diva_ids(bib_database, dataframe):\n",
    "    #parser = BibTexParser(ignore_nonstandard_types=False)\n",
    "    #bib_database = bibtexparser.loads(bibtex_content, parser=parser)\n",
    "    results = {}\n",
    "    FUZZY_MATCH_THRESHOLD = 90\n",
    "    print(\"--- Starting Match Process ---\\n\")\n",
    "    \n",
    "    for entry in bib_database.entries:\n",
    "        bib_key = entry['ID']\n",
    "        print(f\"Processing BibTeX key: {bib_key}\")\n",
    "        found_match = None\n",
    "\n",
    "        # Method 1: Match by Identifiers\n",
    "        for id_type in identifier_cols:\n",
    "            if id_type in entry and f'norm_{id_type}' in dataframe.columns:\n",
    "                norm_bib_id = normalize_identifier(entry[id_type])\n",
    "                \n",
    "                # --- DEBUGGING PRINTS ---\n",
    "                print(f\"  Attempting to match on '{id_type}'...\")\n",
    "                print(f\"    - Raw .bib value:      '{entry.get(id_type)}'\")\n",
    "                print(f\"    - Normalized .bib value: '{norm_bib_id}'\")\n",
    "                \n",
    "                if norm_bib_id:\n",
    "                    id_match = dataframe[dataframe[f'norm_{id_type}'] == norm_bib_id]\n",
    "                    if not id_match.empty:\n",
    "                        diva_id = id_match.iloc[0]['recordInfo.recordIdentifier']\n",
    "                        found_match = (diva_id, f\"Identifier ({id_type})\")\n",
    "                        print(f\"    -> SUCCESS: Found DiVA ID {diva_id}\")\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"    -> SKIPPED: Identifier is empty after normalization.\")\n",
    "        if found_match:\n",
    "            results[bib_key] = {'DiVA_ID': found_match[0], 'match_method': found_match[1]}\n",
    "            continue\n",
    "\n",
    "        # Method 2: Fallback to Fuzzy Title Matching\n",
    "        if 'title' in entry:\n",
    "            norm_bib_title = normalize_title(entry['title'])\n",
    "            # print(f\"{norm_bib_title=}\")\n",
    "            best_score = 0\n",
    "            best_match_id = None\n",
    "            \n",
    "            title_cols_map = {\n",
    "                'eng': ('title.eng', 'subtitle.eng'),\n",
    "                'swe': ('title.swe', 'subtitle.swe')\n",
    "            }\n",
    "            \n",
    "            for lang in title_cols_map.keys():\n",
    "                if f'norm_title_{lang}' in dataframe.columns:\n",
    "                    scores = dataframe[f'norm_title_{lang}'].apply(lambda df_title: fuzz.ratio(norm_bib_title, df_title))\n",
    "                    if scores.max() > best_score:\n",
    "                        best_score = scores.max()\n",
    "                        best_match_id = dataframe.loc[scores.idxmax()]['recordInfo.recordIdentifier']\n",
    "\n",
    "            if best_score > FUZZY_MATCH_THRESHOLD:\n",
    "                found_match = (best_match_id, f\"Fuzzy Title (Score: {best_score})\")\n",
    "        if found_match:\n",
    "            results[bib_key] = {'DiVA_ID': found_match[0], 'match_method': found_match[1]}\n",
    "            continue\n",
    "\n",
    "        # method 3 - brute force check for matching software link\n",
    "        bib_type = entry['ENTRYTYPE']\n",
    "        if bib_type == 'software':\n",
    "            for idx, row in dataframe.iterrows():\n",
    "                #print(f\"{idx=}\")\n",
    "                diva_id=row['recordInfo.recordIdentifier']\n",
    "                d=row['Year']\n",
    "                bib_other_links=row['location.other_links']\n",
    "                bib_url=entry['url']\n",
    "                # 'url': 'https://doi.org/10.5281/zenodo.4435970'\n",
    "                doi_prefix='https://doi.org/'\n",
    "                if bib_url.startswith(doi_prefix):\n",
    "                    bib_pseudo_doi=bib_url[len(doi_prefix):]\n",
    "                # diva2:1527198: ['Software url: https://dl.acm.org/do/10.5281/zenodo.4435970/abs/', 'fulltext:postprint url: https://kth.diva-portal.org/smash/get/diva2:1527198/FULLTEXT01.pdf', 'fulltext:postprint url: https://kth.diva-portal.org/smash/get/diva2:1527198/FULLTEXT02.pdf']\n",
    "                if diva_id == 'diva2:1527198':\n",
    "                    print(f\"{diva_id}: {bib_other_links} {type(bib_other_links)} {bib_pseudo_doi=}\")\n",
    "                if isinstance(bib_other_links, list):\n",
    "                    for ol in bib_other_links:\n",
    "                        #print(f\"{ol=} {type(ol)=}\")\n",
    "                        software_url_prefix='Software url: '\n",
    "                        if ol.startswith(software_url_prefix):\n",
    "                            if ol[len(software_url_prefix):].find(bib_pseudo_doi) > 0:\n",
    "                                found_match = (diva_id, f\"Software link (url: {ol[len(software_url_prefix):]})\")\n",
    "                                break\n",
    "\n",
    "        \n",
    "        if found_match:\n",
    "            results[bib_key] = {'DiVA_ID': found_match[0], 'match_method': found_match[1]}\n",
    "        else:\n",
    "            print(\"  -> FAILED: No match found for this entry.\")\n",
    "            results[bib_key] = {'DiVA_ID': 'Not found in database', 'match_method': 'None'}\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- 4. Run and print final results ---\n",
    "\n",
    "df1=user_df.copy()\n",
    "df1=preprocess_dataframe(df1)\n",
    "found_entries = find_diva_ids(bib_database, df1)\n",
    "\n",
    "print(\"\\n--- Final Matching Results ---\")\n",
    "pprint(found_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication: \tYear  \tpubtype             \tDiVA Id         \tBibliographic key\n",
      "            1\t2019  \tinproceedings       \tdiva2:1291291   \tfarshin_make_2019 \n",
      "            2\t1991  \tarticle             \tdiva2:461420    \tioannidis_coherent_1991 \n",
      "            3\t2016  \tarticle             \tdiva2:948742    \tkim_small-mass_2016 \n",
      "            4\t2014  \tarticle             \tdiva2:690828    \tmaguire_jr_new_2014 \n",
      "            5\t2015  \tinproceedings       \tdiva2:844010    \tbogdanov_nearest_2015 \n",
      "            6\t2013  \tinproceedings       \tdiva2:675824    \troozbeh_resource_2013 \n",
      "            7\t2000  \tpatent              \tdiva2:456465    \tUS7107453B2 \n",
      "            8\t2020  \tpatent              \tdiva2:1926019   \tUS12111768B2 \n",
      "            9\t2022  \tinproceedings       \tdiva2:1609944   \tGhasemirahni_2022 \n",
      "           10\t2021  \tinproceedings       \tdiva2:1527198   \t10.1145/3445814.3446724 \n",
      "           11\t2021  \tsoftware            \tdiva2:1527198   \t10.5281/zenodo.4435970 \n",
      "           12\t1975  \tarticle             \tdiva2:456505    \tNoz_1975 \n",
      "           13\t2023  \tpreprint            \tdiva2:1944107   \tverardo2023fmmheadenhancingautoencoderbasedecg \n",
      "matched a name in Gerald Q. Maguire Jr.\n",
      "\tEntry for publication FakePub2025 not fond in author's publications\n",
      "           14\t2025  \tarticle             \tNot found in database\tFakePub2025 \n",
      "matched a name in Mark T. Smith and Gerald Q. Maguire Jr.\n",
      "\tEntry for publication patentapplication2002 not fond in author's publications\n",
      "           15\t2002  \tpatentapplication   \tNot found in database\tpatentapplication2002 \n",
      "matched a name in Amir Roozbeh and Dejan Kostic and Gerald Q Maguire Jr and Alireza Farshin\n",
      "\tEntry for publication patentapplication2019 not fond in author's publications\n",
      "           16\t2019  \tpatentapplication   \tNot found in database\tpatentapplication2019 \n",
      "           17\t2015  \ttechreport          \tdiva2:866274    \tLee2015 \n",
      "           18\t2003  \tposter              \tdiva2:456494    \tKhatib2003 \n"
     ]
    }
   ],
   "source": [
    "def included_pubs_DiVA_ids(bib_database, found_entries, last_name, first_name):\n",
    "    authors_pubs=dict()\n",
    "    df=pd.DataFrame()           # define a dataframe to collect the results\n",
    "    df2=pd.DataFrame()          # define a dataframe for each entry\n",
    "    print_print_urls=['https://arxiv.org/']\n",
    "\n",
    "    pubyear_field_width=6\n",
    "    pubtype_field_width=20\n",
    "    pub_number_field_width=len('Publication:')+1\n",
    "    DiVAID_field_width=16\n",
    "    print(f\"{'Publication:':{pub_number_field_width}}\\t{'Year':{pubyear_field_width}}\\t{'pubtype':{pubtype_field_width}}\\t{'DiVA Id':{DiVAID_field_width}}\\tBibliographic key\")\n",
    "    pub_number=0\n",
    "\n",
    "    for idx, entry in enumerate(bib_database.entries):\n",
    "        bib_key = entry['ID']\n",
    "        fe=found_entries[bib_key]\n",
    "        did=fe.get('DiVA_ID', None)\n",
    "        if did == 'Not found in database':\n",
    "            # check if the user is an likely to be an author\n",
    "            e_authors=entry.get('author', None)\n",
    "            if e_authors is None:\n",
    "                # print(f\"{idx+1}: No authors for {bib_key}\")\n",
    "                continue\n",
    "            # look for author's last name and (first name or first initial)\n",
    "            if (e_authors.find(last_name) >= 0) and ((e_authors.find(first_name) >= 0) or (e_authors.find(first_name[0]+'.') >= 0)):\n",
    "                print(f\"matched a name in {e_authors}\")\n",
    "                print(f\"\\tEntry for publication {bib_key} not fond in author's publications\")\n",
    "                pubtype=entry.get('ENTRYTYPE', None)\n",
    "                if pubtype == 'misc':\n",
    "                    pub_url=entry.get('url', None)\n",
    "                    for pp_url in print_print_urls:\n",
    "                        if pub_url.startswith(pp_url):\n",
    "                            pubtype='preprint'\n",
    "                if pubtype == 'patent': # if it is a patent but not in DiVA assume it is a patent application\n",
    "                    pubtype='patentapplication'\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            pubtype=entry.get('ENTRYTYPE', None)\n",
    "            if pubtype == 'misc':\n",
    "                pub_url=entry.get('url', None)\n",
    "                for pp_url in print_print_urls:\n",
    "                    if pub_url.startswith(pp_url):\n",
    "                        pubtype='preprint'\n",
    "            if pubtype in ['article', 'inproceedings']:\n",
    "                pub_note=entry.get('note', None)\n",
    "                if pub_note and re.search('poster', pub_note, re.IGNORECASE):\n",
    "                    pubtype='poster'\n",
    "        \n",
    "        pub_number=pub_number+1\n",
    "        pub_year=entry.get('year', None)\n",
    "        pub_title=entry.get('title', None)\n",
    "        print(f\"{pub_number:{pub_number_field_width}}\\t{pub_year:{pubyear_field_width}}\\t{pubtype:{pubtype_field_width}}\\t{did:{DiVAID_field_width}}\\t{bib_key} \")\n",
    "        authors_pubs[pub_number]={'pubtype': pubtype, 'DiVA_id': did, 'bib_key': bib_key, 'year': pub_year, 'title': pub_title}\n",
    "\n",
    "        try:\n",
    "            df2 = pd.json_normalize(authors_pubs[pub_number])\n",
    "        except BaseException as err:\n",
    "            print(f\"Unexpected {err=}, {type(err)=} with {authors_pubs[pub_number]=}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df=pd.concat([df, df2], axis=0, join='outer', ignore_index = True, verify_integrity=False)\n",
    "        except BaseException as err:\n",
    "            print(f\"Unexpected {err=}, {type(err)=} with {df2=}\")\n",
    "            continue\n",
    "\n",
    "    return df.sort_values(by='year')\n",
    "\n",
    "\n",
    "authors_pubs=included_pubs_DiVA_ids(bib_database, found_entries, last_name, first_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubtype</th>\n",
       "      <th>DiVA_id</th>\n",
       "      <th>bib_key</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>article</td>\n",
       "      <td>diva2:456505</td>\n",
       "      <td>Noz_1975</td>\n",
       "      <td>1975</td>\n",
       "      <td>{Calculation of $\\bar{E}_{\\beta}$, $\\Gamma$ an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>article</td>\n",
       "      <td>diva2:461420</td>\n",
       "      <td>ioannidis_coherent_1991</td>\n",
       "      <td>1991</td>\n",
       "      <td>Coherent {File} {Distribution} {Protocol}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>patent</td>\n",
       "      <td>diva2:456465</td>\n",
       "      <td>US7107453B2</td>\n",
       "      <td>2000</td>\n",
       "      <td>Authenticatable graphical bar codes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>patentapplication</td>\n",
       "      <td>Not found in database</td>\n",
       "      <td>patentapplication2002</td>\n",
       "      <td>2002</td>\n",
       "      <td>Location requests by a network device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>poster</td>\n",
       "      <td>diva2:456494</td>\n",
       "      <td>Khatib2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>MobiCom poster : wireless LAN access points as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>diva2:675824</td>\n",
       "      <td>roozbeh_resource_2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>Resource {Monitoring} in a {Network} {Embedded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article</td>\n",
       "      <td>diva2:690828</td>\n",
       "      <td>maguire_jr_new_2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>A {New} {Automated} {Way} to {Measure} {Polyet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>diva2:844010</td>\n",
       "      <td>bogdanov_nearest_2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>The nearest replica can be farther than you think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>techreport</td>\n",
       "      <td>diva2:866274</td>\n",
       "      <td>Lee2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>Dynamic PET visualization of bone remodeling u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>article</td>\n",
       "      <td>diva2:948742</td>\n",
       "      <td>kim_small-mass_2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Do {Small}-{Mass} {Neutrinos} {Participate} in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>patentapplication</td>\n",
       "      <td>Not found in database</td>\n",
       "      <td>patentapplication2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>Memory allocation in a hierarchical memory system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>diva2:1291291</td>\n",
       "      <td>farshin_make_2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>Make the {Most} out of {Last} {Level} {Cache} ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>patent</td>\n",
       "      <td>diva2:1926019</td>\n",
       "      <td>US12111768B2</td>\n",
       "      <td>2020</td>\n",
       "      <td>Methods and devices for controlling memory han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>diva2:1527198</td>\n",
       "      <td>10.1145/3445814.3446724</td>\n",
       "      <td>2021</td>\n",
       "      <td>PacketMill: toward per-Core {100-Gbps} networking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>software</td>\n",
       "      <td>diva2:1527198</td>\n",
       "      <td>10.5281/zenodo.4435970</td>\n",
       "      <td>2021</td>\n",
       "      <td>{PacketMill: Toward Per-Core 100-Gbps Networki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>diva2:1609944</td>\n",
       "      <td>Ghasemirahni_2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>Packet Order Matters! Improving Application Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>preprint</td>\n",
       "      <td>diva2:1944107</td>\n",
       "      <td>verardo2023fmmheadenhancingautoencoderbasedecg</td>\n",
       "      <td>2023</td>\n",
       "      <td>{FMM-Head}: Enhancing Autoencoder-based {ECG} ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>article</td>\n",
       "      <td>Not found in database</td>\n",
       "      <td>FakePub2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>A Fake Publication for testing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pubtype                DiVA_id  \\\n",
       "11            article           diva2:456505   \n",
       "1             article           diva2:461420   \n",
       "6              patent           diva2:456465   \n",
       "14  patentapplication  Not found in database   \n",
       "17             poster           diva2:456494   \n",
       "5       inproceedings           diva2:675824   \n",
       "3             article           diva2:690828   \n",
       "4       inproceedings           diva2:844010   \n",
       "16         techreport           diva2:866274   \n",
       "2             article           diva2:948742   \n",
       "15  patentapplication  Not found in database   \n",
       "0       inproceedings          diva2:1291291   \n",
       "7              patent          diva2:1926019   \n",
       "9       inproceedings          diva2:1527198   \n",
       "10           software          diva2:1527198   \n",
       "8       inproceedings          diva2:1609944   \n",
       "12           preprint          diva2:1944107   \n",
       "13            article  Not found in database   \n",
       "\n",
       "                                           bib_key  year  \\\n",
       "11                                        Noz_1975  1975   \n",
       "1                          ioannidis_coherent_1991  1991   \n",
       "6                                      US7107453B2  2000   \n",
       "14                           patentapplication2002  2002   \n",
       "17                                      Khatib2003  2003   \n",
       "5                            roozbeh_resource_2013  2013   \n",
       "3                              maguire_jr_new_2014  2014   \n",
       "4                            bogdanov_nearest_2015  2015   \n",
       "16                                         Lee2015  2015   \n",
       "2                              kim_small-mass_2016  2016   \n",
       "15                           patentapplication2019  2019   \n",
       "0                                farshin_make_2019  2019   \n",
       "7                                     US12111768B2  2020   \n",
       "9                          10.1145/3445814.3446724  2021   \n",
       "10                          10.5281/zenodo.4435970  2021   \n",
       "8                                Ghasemirahni_2022  2022   \n",
       "12  verardo2023fmmheadenhancingautoencoderbasedecg  2023   \n",
       "13                                     FakePub2025  2025   \n",
       "\n",
       "                                                title  \n",
       "11  {Calculation of $\\bar{E}_{\\beta}$, $\\Gamma$ an...  \n",
       "1           Coherent {File} {Distribution} {Protocol}  \n",
       "6                 Authenticatable graphical bar codes  \n",
       "14              Location requests by a network device  \n",
       "17  MobiCom poster : wireless LAN access points as...  \n",
       "5   Resource {Monitoring} in a {Network} {Embedded...  \n",
       "3   A {New} {Automated} {Way} to {Measure} {Polyet...  \n",
       "4   The nearest replica can be farther than you think  \n",
       "16  Dynamic PET visualization of bone remodeling u...  \n",
       "2   Do {Small}-{Mass} {Neutrinos} {Participate} in...  \n",
       "15  Memory allocation in a hierarchical memory system  \n",
       "0   Make the {Most} out of {Last} {Level} {Cache} ...  \n",
       "7   Methods and devices for controlling memory han...  \n",
       "9   PacketMill: toward per-Core {100-Gbps} networking  \n",
       "10  {PacketMill: Toward Per-Core 100-Gbps Networki...  \n",
       "8   Packet Order Matters! Improving Application Pe...  \n",
       "12  {FMM-Head}: Enhancing Autoencoder-based {ECG} ...  \n",
       "13                     A Fake Publication for testing  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_of_publications(authors_pubs):\n",
    "    gen_txt=\"\"\n",
    "    categories=['ListOfPapers',\n",
    "                'ListOfPatents',\n",
    "                'ListOfArtifacts',\n",
    "                'ListOfPosters',\n",
    "                'ListOfPatentApplications',\n",
    "                'ListOfReports',\n",
    "                'ListOfDatasets'\n",
    "]\n",
    "    for c in categories:\n",
    "        c_txt=\"\"\n",
    "        c_index=0\n",
    "        for idx, row in authors_pubs.iterrows():\n",
    "            #print(f\"{c} {row['pubtype']}\")\n",
    "            if c == 'ListOfPapers' and row['pubtype'] in ['article', 'preprint', 'conference', 'inproceedings', 'incollection',\n",
    "                                                          'thesis', 'mastersthesis', 'phdthesis', \n",
    "                                                          'book', 'inbook', 'mvbook', 'periodical',\n",
    "                                                          'collection', 'mvcollection', 'proceedings', 'mvproceedings',\n",
    "                                                          'inreference',\n",
    "                                                        ]:\n",
    "                c_index_char=chr(ord('A')+c_index)\n",
    "                c_index=c_index+1\n",
    "                c_note=\"\"\n",
    "                if row['pubtype'] == 'preprint':\n",
    "                    c_note=\" (preprint)\"\n",
    "                c_txt=c_txt+\"\\\\item \\\\label{paper:\"+f\"{c_index_char}\" +\"} \" + f\"{row['title']}\\\\cite\" + \"{\" +f\"{row['bib_key']}\" + \"}\"+f\"{c_note}\"+\"\\n\"\n",
    "            elif c == 'ListOfPatents' and row['pubtype'] in ['patent']:\n",
    "                c_index_char=chr(ord('A')+c_index)\n",
    "                c_index=c_index+1\n",
    "                c_txt=c_txt+\"\\\\item \\\\label{patent:\"+f\"{c_index_char}\" +\"} \" + f\"{row['title']}\\\\cite\" + \"{\" +f\"{row['bib_key']}\" + \"}\\n\"  \n",
    "            elif c == 'ListOfArtifacts' and row['pubtype'] in ['software', 'unpublished']:\n",
    "                c_index_char=chr(ord('A')+c_index)\n",
    "                c_index=c_index+1\n",
    "                c_txt=c_txt+\"\\\\item \\\\label{artifact:\"+f\"{c_index_char}\" +\"} \" + f\"{row['title']}\\\\cite\" + \"{\" +f\"{row['bib_key']}\" + \"}\\n\"\n",
    "            elif c == 'ListOfDatasets' and row['pubtype'] in ['dataset']:\n",
    "                c_index_char=chr(ord('A')+c_index)\n",
    "                c_index=c_index+1\n",
    "                c_txt=c_txt+\"\\\\item \\\\label{dataset:\"+f\"{c_index_char}\" +\"} \" + f\"{row['title']}\\\\cite\" + \"{\" +f\"{row['bib_key']}\" + \"}\\n\"\n",
    "            elif c == 'ListOfPatentApplications' and row['pubtype'] in ['patentapplication']:\n",
    "                c_index_char=chr(ord('A')+c_index)\n",
    "                c_index=c_index+1\n",
    "                c_txt=c_txt+\"\\\\item \\\\label{patentapplication:\"+f\"{c_index_char}\" +\"} \" + f\"{row['title']}\\\\cite\" + \"{\" +f\"{row['bib_key']}\" + \"}\\n\"  \n",
    "            elif c == 'ListOfReports' and row['pubtype'] in ['report', 'techreport', 'manual', 'booklet', 'suppbook', 'suppcollection',\n",
    "                                                             'suppperiodical', 'online', 'electronic', 'misc', 'www'\n",
    "                                                            ]:\n",
    "                c_index_char=chr(ord('A')+c_index)\n",
    "                c_index=c_index+1\n",
    "                c_txt=c_txt+\"\\\\item \\\\label{report:\"+f\"{c_index_char}\" +\"} \" + f\"{row['title']}\\\\cite\" + \"{\" +f\"{row['bib_key']}\" + \"}\\n\"\n",
    "            elif c == 'ListOfPosters' and row['pubtype'] in ['poster']:\n",
    "                c_index_char=chr(ord('A')+c_index)\n",
    "                c_index=c_index+1\n",
    "                c_txt=c_txt+\"\\\\item \\\\label{poster:\"+f\"{c_index_char}\" +\"} \" + f\"{row['title']}\\\\cite\" + \"{\" +f\"{row['bib_key']}\" + \"}\\n\"    \n",
    "    \n",
    "            #print(c_txt)\n",
    "        if len(c_txt) > 0:\n",
    "            gen_txt=gen_txt+\"\\\\begin{\" + f\"{c}\" + \"}\\n\" + c_txt + \"\\\\end{\" + f\"{c}\" + \"}\\n\"\n",
    "    # add blank lists for thing not included in thesis\n",
    "    gen_txt=gen_txt+'%%%%%%%%%\\n% place holders for lists of publications _not_ included in thesis\\n'\n",
    "    for c in categories:\n",
    "        gen_txt=gen_txt+\"\\\\begin{\" + f\"{c}\" + \"}\\n\" + \"\\\\end{\" + f\"{c}\" + \"}\\n\"\n",
    "    print(gen_txt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{ListOfPapers}\n",
      "\\item \\label{paper:A} {Calculation of $\\bar{E}_{\\beta}$, $\\Gamma$ and $\\Delta_{i}$ for $^{99m}$Tc}\\cite{Noz_1975}\n",
      "\\item \\label{paper:B} Coherent {File} {Distribution} {Protocol}\\cite{ioannidis_coherent_1991}\n",
      "\\item \\label{paper:C} Resource {Monitoring} in a {Network} {Embedded} {Cloud}: {An} {Extension} to {OSPF}-{TE}\\cite{roozbeh_resource_2013}\n",
      "\\item \\label{paper:D} A {New} {Automated} {Way} to {Measure} {Polyethylene} {Wear} in {THA} {Using} a {High} {Resolution} {CT} {Scanner}: {Method} and {Analysis}\\cite{maguire_jr_new_2014}\n",
      "\\item \\label{paper:E} The nearest replica can be farther than you think\\cite{bogdanov_nearest_2015}\n",
      "\\item \\label{paper:F} Do {Small}-{Mass} {Neutrinos} {Participate} in {Gauge} {Transformations}?\\cite{kim_small-mass_2016}\n",
      "\\item \\label{paper:G} Make the {Most} out of {Last} {Level} {Cache} in {Intel} {Processors}\\cite{farshin_make_2019}\n",
      "\\item \\label{paper:H} PacketMill: toward per-Core {100-Gbps} networking\\cite{10.1145/3445814.3446724}\n",
      "\\item \\label{paper:I} Packet Order Matters! Improving Application Performance by Deliberately Delaying Packets\\cite{Ghasemirahni_2022}\n",
      "\\item \\label{paper:J} {FMM-Head}: Enhancing Autoencoder-based {ECG} anomaly detection with prior knowledge\\cite{verardo2023fmmheadenhancingautoencoderbasedecg} (preprint)\n",
      "\\item \\label{paper:K} A Fake Publication for testing\\cite{FakePub2025}\n",
      "\\end{ListOfPapers}\n",
      "\\begin{ListOfPatents}\n",
      "\\item \\label{patent:A} Authenticatable graphical bar codes\\cite{US7107453B2}\n",
      "\\item \\label{patent:B} Methods and devices for controlling memory handling\\cite{US12111768B2}\n",
      "\\end{ListOfPatents}\n",
      "\\begin{ListOfArtifacts}\n",
      "\\item \\label{artifact:A} {PacketMill: Toward Per-Core 100-Gbps Networking - Artifact for ASPLOS'21}\\cite{10.5281/zenodo.4435970}\n",
      "\\end{ListOfArtifacts}\n",
      "\\begin{ListOfPosters}\n",
      "\\item \\label{poster:A} MobiCom poster : wireless LAN access points as queuing systems: performance analysis and service time\\cite{Khatib2003}\n",
      "\\end{ListOfPosters}\n",
      "\\begin{ListOfPatentApplications}\n",
      "\\item \\label{patentapplication:A} Location requests by a network device\\cite{patentapplication2002}\n",
      "\\item \\label{patentapplication:B} Memory allocation in a hierarchical memory system\\cite{patentapplication2019}\n",
      "\\end{ListOfPatentApplications}\n",
      "\\begin{ListOfReports}\n",
      "\\item \\label{report:A} Dynamic PET visualization of bone remodeling using NaF-18\\cite{Lee2015}\n",
      "\\end{ListOfReports}\n",
      "%%%%%%%%%\n",
      "% place holders for lists of publications _not_ included in thesis\n",
      "\\begin{ListOfPapers}\n",
      "\\end{ListOfPapers}\n",
      "\\begin{ListOfPatents}\n",
      "\\end{ListOfPatents}\n",
      "\\begin{ListOfArtifacts}\n",
      "\\end{ListOfArtifacts}\n",
      "\\begin{ListOfPosters}\n",
      "\\end{ListOfPosters}\n",
      "\\begin{ListOfPatentApplications}\n",
      "\\end{ListOfPatentApplications}\n",
      "\\begin{ListOfReports}\n",
      "\\end{ListOfReports}\n",
      "\\begin{ListOfDatasets}\n",
      "\\end{ListOfDatasets}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_list_of_publications(authors_pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_of_publicationsv2(authors_pubs):\n",
    "    # 1. Define the categories in the desired final order, as you suggested.\n",
    "    categories = [\n",
    "        'ListOfPapers',\n",
    "        'ListOfPatents',\n",
    "        'ListOfArtifacts',\n",
    "        'ListOfPosters',\n",
    "        'ListOfPatentApplications',\n",
    "        'ListOfReports',\n",
    "        'ListOfDatasets'\n",
    "    ]\n",
    "\n",
    "    # 2. Define which publication types belong to each category.\n",
    "    category_map = {\n",
    "        'ListOfPapers': ['article', 'preprint', 'conference', 'inproceedings', 'incollection', 'thesis', 'mastersthesis', 'phdthesis', 'book', 'inbook', 'mvbook', 'periodical', 'collection', 'mvcollection', 'proceedings', 'mvproceedings', 'inreference'],\n",
    "        'ListOfPatents': ['patent'],\n",
    "        'ListOfArtifacts': ['software', 'unpublished'],\n",
    "        'ListOfPosters': ['poster'],\n",
    "        'ListOfPatentApplications': ['patentapplication'],\n",
    "        'ListOfReports': ['report', 'techreport', 'manual', 'booklet', 'suppbook', 'suppcollection', 'suppperiodical', 'online', 'electronic', 'misc', 'www'],\n",
    "        'ListOfDatasets': ['dataset']\n",
    "    }\n",
    "\n",
    "    # 3. Initialize the dictionary with empty lists, preserving order.\n",
    "    categorized_items = {cat: [] for cat in categories}\n",
    "    \n",
    "    # --- Helper function to format each item ---\n",
    "    def format_latex_item(row, category_name, current_index):\n",
    "        label_prefix = category_name.replace('ListOf', '').lower()\n",
    "        if label_prefix.endswith('s'):\n",
    "            label_prefix = label_prefix[:-1]\n",
    "        \n",
    "        c_index_char = chr(ord('A') + current_index)\n",
    "        c_note = \" (preprint)\" if row['pubtype'] == 'preprint' else \"\"\n",
    "        \n",
    "        return f\"\\\\item \\\\label{{{label_prefix}:{c_index_char}}} {row['title']}\\\\cite{{{row['bib_key']}}}{c_note}\\n\"\n",
    "\n",
    "    # --- Main Loop: Iterate through publications just ONCE ---\n",
    "    for idx, row in authors_pubs.iterrows():\n",
    "        pub_type = row['pubtype']\n",
    "        for category_name, type_list in category_map.items():\n",
    "            if pub_type in type_list:\n",
    "                # Get the current number of items in this category to determine the next index\n",
    "                current_item_count = len(categorized_items[category_name])\n",
    "                formatted_item = format_latex_item(row, category_name, current_item_count)\n",
    "                categorized_items[category_name].append(formatted_item)\n",
    "                break # Found the correct category, move to the next publication\n",
    "\n",
    "    # --- Generate the final output string, maintaining the correct order ---\n",
    "    gen_txt = \"\"\n",
    "    for category in categories:\n",
    "        items = categorized_items[category]\n",
    "        if items:\n",
    "            gen_txt += f\"\\\\begin{{{category}}}\\n\"\n",
    "            gen_txt += \"\".join(items)\n",
    "            gen_txt += f\"\\\\end{{{category}}}\\n\"\n",
    "\n",
    "    # Add the empty placeholders for the user to fill\n",
    "    gen_txt += '%%%%%%%%%\\n% place holders for lists of publications _not_ included in thesis\\n'\n",
    "    for category in categories:\n",
    "        gen_txt += f\"\\\\begin{{{category}}}\\n\\\\end{{{category}}}\\n\"\n",
    "        \n",
    "    print(gen_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{ListOfPapers}\n",
      "\\item \\label{paper:A} {Calculation of $\\bar{E}_{\\beta}$, $\\Gamma$ and $\\Delta_{i}$ for $^{99m}$Tc}\\cite{Noz_1975}\n",
      "\\item \\label{paper:B} Coherent {File} {Distribution} {Protocol}\\cite{ioannidis_coherent_1991}\n",
      "\\item \\label{paper:C} Resource {Monitoring} in a {Network} {Embedded} {Cloud}: {An} {Extension} to {OSPF}-{TE}\\cite{roozbeh_resource_2013}\n",
      "\\item \\label{paper:D} A {New} {Automated} {Way} to {Measure} {Polyethylene} {Wear} in {THA} {Using} a {High} {Resolution} {CT} {Scanner}: {Method} and {Analysis}\\cite{maguire_jr_new_2014}\n",
      "\\item \\label{paper:E} The nearest replica can be farther than you think\\cite{bogdanov_nearest_2015}\n",
      "\\item \\label{paper:F} Do {Small}-{Mass} {Neutrinos} {Participate} in {Gauge} {Transformations}?\\cite{kim_small-mass_2016}\n",
      "\\item \\label{paper:G} Make the {Most} out of {Last} {Level} {Cache} in {Intel} {Processors}\\cite{farshin_make_2019}\n",
      "\\item \\label{paper:H} PacketMill: toward per-Core {100-Gbps} networking\\cite{10.1145/3445814.3446724}\n",
      "\\item \\label{paper:I} Packet Order Matters! Improving Application Performance by Deliberately Delaying Packets\\cite{Ghasemirahni_2022}\n",
      "\\item \\label{paper:J} {FMM-Head}: Enhancing Autoencoder-based {ECG} anomaly detection with prior knowledge\\cite{verardo2023fmmheadenhancingautoencoderbasedecg} (preprint)\n",
      "\\item \\label{paper:K} A Fake Publication for testing\\cite{FakePub2025}\n",
      "\\end{ListOfPapers}\n",
      "\\begin{ListOfPatents}\n",
      "\\item \\label{patent:A} Authenticatable graphical bar codes\\cite{US7107453B2}\n",
      "\\item \\label{patent:B} Methods and devices for controlling memory handling\\cite{US12111768B2}\n",
      "\\end{ListOfPatents}\n",
      "\\begin{ListOfArtifacts}\n",
      "\\item \\label{artifact:A} {PacketMill: Toward Per-Core 100-Gbps Networking - Artifact for ASPLOS'21}\\cite{10.5281/zenodo.4435970}\n",
      "\\end{ListOfArtifacts}\n",
      "\\begin{ListOfPosters}\n",
      "\\item \\label{poster:A} MobiCom poster : wireless LAN access points as queuing systems: performance analysis and service time\\cite{Khatib2003}\n",
      "\\end{ListOfPosters}\n",
      "\\begin{ListOfPatentApplications}\n",
      "\\item \\label{patentapplication:A} Location requests by a network device\\cite{patentapplication2002}\n",
      "\\item \\label{patentapplication:B} Memory allocation in a hierarchical memory system\\cite{patentapplication2019}\n",
      "\\end{ListOfPatentApplications}\n",
      "\\begin{ListOfReports}\n",
      "\\item \\label{report:A} Dynamic PET visualization of bone remodeling using NaF-18\\cite{Lee2015}\n",
      "\\end{ListOfReports}\n",
      "%%%%%%%%%\n",
      "% place holders for lists of publications _not_ included in thesis\n",
      "\\begin{ListOfPapers}\n",
      "\\end{ListOfPapers}\n",
      "\\begin{ListOfPatents}\n",
      "\\end{ListOfPatents}\n",
      "\\begin{ListOfArtifacts}\n",
      "\\end{ListOfArtifacts}\n",
      "\\begin{ListOfPosters}\n",
      "\\end{ListOfPosters}\n",
      "\\begin{ListOfPatentApplications}\n",
      "\\end{ListOfPatentApplications}\n",
      "\\begin{ListOfReports}\n",
      "\\end{ListOfReports}\n",
      "\\begin{ListOfDatasets}\n",
      "\\end{ListOfDatasets}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_list_of_publicationsv2(authors_pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "288156dbef364610b068b4d88c886f95",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
